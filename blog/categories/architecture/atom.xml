<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Architecture | Hakan Tuncer's Blog]]></title>
  <link href="http://www.hakantuncer.com/blog/categories/architecture/atom.xml" rel="self"/>
  <link href="http://www.hakantuncer.com/"/>
  <updated>2017-07-15T15:12:48+02:00</updated>
  <id>http://www.hakantuncer.com/</id>
  <author>
    <name><![CDATA[Hakan Tuncer]]></name>
    <email><![CDATA[hakantuncer@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting Up Continuous Delivery for a Node.js REST API - Part 2]]></title>
    <link href="http://www.hakantuncer.com/blog/2017/03/25/setting-up-continuous-delivery-for-a-node-dot-js-rest-api-part-2/"/>
    <updated>2017-03-25T09:48:02+01:00</updated>
    <id>http://www.hakantuncer.com/blog/2017/03/25/setting-up-continuous-delivery-for-a-node-dot-js-rest-api-part-2</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="/blog/2017/02/17/setting-up-continuous-integration-and-delivery-for-a-nodejs-rest-api/">In the first part</a> of this blog post, I shared some fundamental ideas that form as background information for what I want to achieve here. So if you haven’t read that one yet, I recommend you to <a href="/blog/2017/02/17/setting-up-continuous-integration-and-delivery-for-a-nodejs-rest-api/">check it out</a>.</p>

<p>A few months back, I decided to convert my <a href="https://github.com/hakant/HackathonPlannerAPI">hobby project — Hackathon Planner API</a> from pure Javascript to TypeScript and <a href="/blog/2016/11/08/converting-a-node-dot-js-express-api-to-typescript/">wrote a blog post about it</a>. This time I sat down to build an Automated Test Suite and a <a href="https://www.martinfowler.com/articles/continuousIntegration.html">Continuous Delivery (CD)</a> pipeline around it.</p>

<p>Effective automated testing is a natural prerequisite for Continuous Integration &amp; Delivery. How can it not be? If you’re not getting quick and broad feedback from your software, how can you delivery frequently? So having a testable architecture and an effective test strategy is very crucial. Let’s dive in.</p>

<h2>Right Architecture for Subcutaneous Testing</h2>

<p>I’m a huge fan of <a href="https://martinfowler.com/bliki/SubcutaneousTest.html">subcutaneous testing</a>. This type of testing starts right under the UI layer with a large scope (which preferably spans all the way down to the data store) can have a great return on investment. On <a href="https://m.facebook.com/notes/kent-beck/making-making-manifesto/857477870951745/">Kent Beck’s feedback chart</a> it would score high up and to the right: <strong>fast and broad feedback</strong>.</p>

<p><img src="/assets/ContinuousIntegration_Part2/Kent_Beck_Feedback_Chart.png" alt="Kent Beck's Feedback Chart" /></p>

<p>So now that we want to target our automated tests below the delivery mechanism layer (UI, Network etc.), this is where an important architectural decision comes into question. What is the main boundary of my application? Where does my significant business logic start? Plus, how can I make my application boundary very visible and clear to all developers? If you follow down this path of thinking, one nice place to end up is a combination of <a href="https://en.wikipedia.org/wiki/Command_pattern">command</a> and <a href="https://en.wikipedia.org/wiki/Mediator_pattern">mediator</a> patterns.</p>

<p>The combination of these patterns is about sending commands down through a very narrow facade, where every command has a very clean input and output “<a href="https://en.wikipedia.org/wiki/Data_transfer_object">data transfer objects</a>” or POCO’s, POJO’s.. whatever you like to call them depending on your stack of choice. They’re simply objects that carry data and no behavior. </p>

<p>To be able to use this pattern in <a href="https://github.com/hakant/HackathonPlannerAPI">my REST API</a>, I’ve created a simple module in TypeScript that allows me to execute commands and optionally get results from them. It’s here: <a href="https://github.com/hakant/TypeScriptCommandPattern">TypeScriptCommandPattern</a>.</p>

<p>In the example below, you can see how a test request is sent down to the executor and a result is returned back.</p>

<br/>


<script src="https://gist.github.com/hakant/837f08708feed35b2d02b99269f0e916.js"></script>


<p>And here is how a “Hello World” style command handler looks like. Notice that it has a clean request and response definitions. At the end of the implementation we make sure that the handler is registered and mapped to the request. In this structure handlers are singleton objects and they can later be resolved by the type of the request and then get executed.</p>

<br/>


<script src="https://gist.github.com/hakant/c4c29c9d6e98fb3a85c674a4b0ad6b28.js"></script>


<h2>Using this pattern in a real world Node.js REST API</h2>

<p>One great architectural benefit of this pattern is that it allows you to nicely separate an application into many independent scenarios — <a href="https://lostechies.com/jimmybogard/2015/07/02/ndc-talk-on-solid-in-slices-not-layers-video-online/">vertical slices</a>. Take a look at <a href="https://github.com/hakant/HackathonPlannerAPI/tree/master/scenarios">these scenarios from Hackathon Planner</a>:</p>

<p><img src="/assets/ContinuousIntegration_Part2/Hackathon_Planner_Scenarios.png" alt="Kent Beck's Feedback Chart" /></p>

<p>Each of these TypeScript modules are a vertical slice. They’re the full story. Yes they make use of other external modules when necessary, but when developers read through these scenarios, they get the full picture. And if code sharing between these scenarios are done wisely (with correct abstractions), then a change in one scenario is contained and does not necessarily affect any others.</p>

<p>Let’s take a look at the implementation of one of these scenarios and extract some key properties. Let’s pick the scenario in “GetIdeas.ts”:</p>

<br/>


<script src="https://gist.github.com/hakant/6c38b895807a4d721c8c367a5117826f.js"></script>


<ul>
<li><p>Line 3–4: Imports the base async command handling structure and also the container to register itself at the end of the file.</p></li>
<li><p>Line 6–9: External modules that this handler uses.</p></li>
<li><p>Line 11–12: A module that is used by multiple scenarios. Be careful, I say “used” not “reused”. Remember the “<a href="http://udidahan.com/2009/06/07/the-fallacy-of-reuse/">fallacy of reuse</a>”. This module (IdeaPrePostProcessor) is used to massage and sanitize the Idea entities before they’re sent to the user interface and before they’re written to the database. Multiple scenarios use this module the exact same way and for the same need. There are no slight variations per scenario, that’s why it’s a right abstraction and that’s why it’s “use”, not “reuse”.</p></li>
<li><p>Line 14–16: Since our handler is an async one (because it’s accessing the database), it implements the AsyncCommandHandler&lt;TRequest, TResponse> and implements the HandleAsync method that returns a Promise of GetIdeasReponse object.</p></li>
<li><p>Line 17–38: Complete vertical implementation of the handler. It’s basically scanning the database for all the ideas, sorting them based on an algorithm, encapsulating them in the response object and return. Notice that at line 25, the “await” keyword is used. It’s a simple and very readable way of representing asynchronous code. It exists in recent versions of TypeScript as well as in ES7.</p></li>
<li><p>Line 42–47: Request and response objects that are used by this handler are defined and exported. These objects should be available to the rest of the application as well.</p></li>
<li><p>Line 50–52: A singleton instance of the handler is instantiated and registered to the type of the request object. Based on application needs, this can be made more complex and sophisticated. Statically typed languages incorporate lots of ideas around IoC containers whereas dynamically typed languages like javascript, not so much. Also keep in mind that these TypeScript generic constructs only exist at design time. They don’t exist in the transpiled javascript and that makes it impossible to discover or scan these constructs at runtime.</p></li>
</ul>


<h2>Simple &amp; Stupid ExpressJS Routes</h2>

<p>If you have experience with building MVC type web applications or REST API’s you’re probably familiar with the idea of controllers. Routes are the same concept in <a href="http://expressjs.com">ExpressJS</a>. Keeping your routes and controllers nice and clean is a good discipline to have. Remember, we want to contain our core application logic and try not to leak it outside as much as possible. Not to frameworks, not to external libraries.</p>

<p>Hackathon Planner’s most significant route is the Ideas route where ideas are being CRUD and a few further actions are taken against them. Code below shows the routes defined in there. Notice how clean it reads and how all business logic is kept out of it. The only thing these routes do is to pass requests and responses up and down the stream.</p>

<br/>


<script src="https://gist.github.com/hakant/85fbfa3f3d77f82c2b17457a96356d09.js"></script>


<h2>Testing all the Scenarios</h2>

<p>The structure of the software created so far lends itself nicely to the subcutaneous testing style discussed earlier. So I take advantage of this by taking the following actions in the tests:</p>

<ul>
<li><p>Remove the ExpressJS layer that normally sits on top. This is a delivery mechanism and I don’t necessarily need it to test my business logic — since I’ve already separated that logic clearly out of this layer.</p></li>
<li><p>Load all scenario handlers together with the real data store (or a realistic emulator).</p></li>
<li><p>Shoot requests &amp; verify responses.</p></li>
<li><p>In some cases, if verifying the response alone isn&rsquo;t sufficient, go further and directly check the data store for verifying the side effects. In my case it wasn’t necessary, I could both act and verify using my handlers alone.</p></li>
</ul>


<p>Let’s take a look at a few of these tests. Below is a part of <a href="https://jasmine.github.io/">jasmine</a> spec that tests inserting and fetching ideas by running several scenarios.</p>

<br/>


<script src="https://gist.github.com/hakant/a2d1c53fdaa5e7c0d99eeb8e8b4022e2.js"></script>


<p>Here are a few key properties of these tests:</p>

<ul>
<li><p>Lines 17–26: Before each test new NoSql tables are created and after each test they’re dropped. This makes every test run in a sandbox isolated from one another. So they can all run in any random order. In general, this is a characteristic of a unit test, not an integration test. So here we have the best of both worlds!</p></li>
<li><p>All tests run against a DynamoDb local emulator. So the feedback coming from the database is real. I trust that the Amazon team behind DynamoDb makes sure that the emulator behaves exactly the same as the real one in the cloud.</p></li>
<li><p>Integration tests are slow right? No, not if you don’t test against user interfaces, or have many number of network calls, or use slow data storage devices. These tests don’t get into any complexity of testing against a UI; they don’t bring up a web server and make excessive number of network calls; and they make use of an in memory database emulator running on the same box. These 15 tests run in ~2 seconds even though each of them setup and tear down their data tables. So you can run quite a lot of these in a few minutes.</p></li>
<li><p>By their nature, integration tests are broad. They give broad feedback. But this is a trade off because when they fail, you have to dig a little deeper to understand exactly what failed in comparison to the unit tests that are tiny and focused. But that’s a tradeoff I like to make in general.</p></li>
<li><p>Having said that, I’m by no means trying to trash the value of unit tests here. If I see the need — like any piece of code that is significant for the system and does interesting things — I’ll go and unit test that part in isolation. Especially code that’s heavy on algorithmic work. So unit tests are valuable when they’re implemented for the right code against the right abstraction.</p></li>
<li><p>Last but not least, these type of tests (i.e subcutaneous) interacts with the system right at the outside of the significant boundary. This means as long as feature requirements don’t change, these tests remain valid and useful. They can survive large refactorings because they’re not coupled to the implementation details! This is a huge deal in my opinion.</p></li>
</ul>


<p>In next and the last part of this series, I would like to write a few things about my experience with setting up a CD pipeline using <a href="https://circleci.com">CircleCI</a>, which transpiles all TypeScript files, installs dependencies, runs tests and deploys to <a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk</a>.</p>

<p>Thanks for reading.</p>

<p>~ Hakan</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up Continuous Delivery for a Node.js REST API - Part 1]]></title>
    <link href="http://www.hakantuncer.com/blog/2017/02/17/setting-up-continuous-integration-and-delivery-for-a-nodejs-rest-api/"/>
    <updated>2017-02-17T22:01:28+01:00</updated>
    <id>http://www.hakantuncer.com/blog/2017/02/17/setting-up-continuous-integration-and-delivery-for-a-nodejs-rest-api</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>A few months back, I decided to convert my <a href="https://github.com/hakant/HackathonPlannerAPI">hobby project &ndash; Hackathon Planner API</a>
from pure Javascript to <a href="https://www.typescriptlang.org/">TypeScript</a> and <a href="/blog/2016/11/08/converting-a-node-dot-js-express-api-to-typescript/">wrote a blog post about it</a>.
This time I sat down to build an Automated Test Suite and a Continuous Delivery (CD) pipeline around it.</p>

<p>Hackathon Planner API is essentially a REST API written in <a href="https://nodejs.org/en/">Node.js</a> and <a href="http://expressjs.com/">Express</a>.
Stores its data in a NOSQL database (<a href="https://aws.amazon.com/dynamodb/">DynamoDB</a>) and hosted on
<a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk</a>.</p>

<p>I thought I would best write this up in 2 blog posts &ndash; due to the fact that most of what I did in this repository
is based on some fundamental concepts which I&rsquo;ve been reading and thinking about in the last few years.</p>

<p>In this post, I would like to touch on a few of these ideas that guided me in my refactoring decisions for
making the codebase cleaner, less coupled and more testable.</p>

<p>In the next post, I&rsquo;ll share some more details and examples from the codebase.</p>

<h2>Enabling Continuous Integation &amp; Delivery</h2>

<p>If you are not very familiar with the terms &ldquo;Continuous Integration&rdquo; (CI) or &ldquo;Continuous Delivery&rdquo; (CD) I suggest checking out
<a href="https://www.thoughtworks.com/continuous-integration">ThoughtWorks website</a>
and <a href="https://www.martinfowler.com/articles/continuousIntegration.html">Martin Fowler&rsquo;s post</a> to learn more about
these software development practices.</p>

<p>I&rsquo;ve intentionally used the word &ldquo;enabling&rdquo; in the title above. I&rsquo;ve seen many brownfield projects
where the team wants to start practicing CI; however, depending on the age
and size of the project this may turn out to be <strong>very difficult, sometimes even impossible</strong>. These are not
practices that can be applied to a project only from the outside. They can&rsquo;t always be easily introduced as
an afterthought either, they have to be built and enabled inside out; and it takes time &amp; effort to get there!</p>

<p>The way I see it; practicing CI or CD are kind of a &ldquo;high performance state&rdquo; a team reaches after getting A
LOT OF THINGS RIGHT; especially around system architecture and automated testing. Skills of the team members
also matter, big time! Committing to trunk everyday without breaking stuff and more importantly, always introducing
a right amount of tests at each new check-in is not an easy task for everyone.</p>

<h2>Clean Architecture</h2>

<p>One of the important lessons I learned from Robert C. Martin&rsquo;s talk <a href="https://vimeo.com/97530863">Clean Architecture and Design</a>
(more resources <a href="https://8thlight.com/blog/uncle-bob/2012/08/13/the-clean-architecture.html">here</a>) is that, it&rsquo;s
almost always a good idea to keep your business logic, the heart of your system, clean from external concerns like
frameworks and delivery mechanisms (web frameworks, native application hosts, operating systems etc.).</p>

<p>This is beneficial in many ways. Keeps your business logic clean and free from the complexities of the environment around
it. It also enables great automated testing capabilities which is probably the most important prerequisite of
&ldquo;Continuous Integration&rdquo; way of working and delivering software.</p>

<p>Btw, just because I&rsquo;m referring to Robert C. Martin&rsquo;s clean architecture here doesn&rsquo;t mean I agree with everything
he&rsquo;s suggesting there &ndash; like f.ex his claim on: &ldquo;storage is an insignificant detail&rdquo;. I&rsquo;d still like to think of
storage interaction as an integral part of my system. More on that in a bit.</p>

<h2>Vertical Slices instead of Horizontal Layers</h2>

<p>We&rsquo;ve all built <a href="https://en.wikipedia.org/wiki/Multitier_architecture">n-tier applications</a> where software is structured
in horizontal layers &ndash; most typical layers being Presentation, Business and Data Access. I&rsquo;ve built many
applications in this way and it&rsquo;s almost certain that I&rsquo;ll come across many others in the future.</p>

<p>What&rsquo;s the promise of n-tier and how did it become so widespread? I think this sentence
<a href="https://en.wikipedia.org/wiki/Multitier_architecture">from wikipedia</a> sums it up:</p>

<blockquote><p>N-tier application architecture provides a model by which developers can create flexible and
reusable applications.</p></blockquote>

<p>In recent years; gotten tired of repeating problems introduced by organically grown n-tier applications &ndash; like
tight-coupling, bloated classes or services with multiple responsibilities,
<a href="https://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction">organically introduced premature/wrong abstractions</a>;
all leading up to code that has become unreadable and hard to reason about.
You know you&rsquo;re in a &ldquo;generic, reusable n-tier application&rdquo; when you have to jump to definitions up and down
OVER AND OVER AGAIN in order get a slightest clue about what a specific scenario is trying to achieve.</p>

<p>Luckily, there are alternative ideas in the community. One of my favorites is the direction <a href="https://twitter.com/jbogard">Jimmy Bogard</a> is taking
in his talk <a href="https://lostechies.com/jimmybogard/2015/07/02/ndc-talk-on-solid-in-slices-not-layers-video-online/">SOLID in Slices not Layers</a>,
and his library <a href="https://github.com/jbogard/MediatR">MediatR</a> that I&rsquo;ve come to learn and love in the
last few years.</p>

<p>I&rsquo;ve built a few applications using MediatR where I implemented all scenarios (think of these as endpoints in a
REST API) in vertical slices and kept the shared code between them to a minimum. I really enjoyed the
outcome. Readability, <a href="https://en.wikipedia.org/wiki/Cohesion_(computer_science)" >cohesion</a> and
<a href="https://lostechies.com/jimmybogard/2016/10/24/vertical-slice-test-fixtures-for-mediatr-and-asp-net-core/">testability</a>
of these applications went really up.</p>

<p>Recently I listened <a href="https://www.dotnetrocks.com/?show=1405">Scott Allen on a podcast</a> where he mentioned
he&rsquo;s also a fan of vertical slicing and he has a <a href="http://odetocode.com/blogs/scott/archive/2016/11/29/addfeaturefolders-and-usenodemodules-on-nuget-for-asp-net-core.aspx">blog post on a related idea</a>.</p>

<p>One other lecture I recommend seeing is by Udi Dahan from NDC Oslo 2016: <a href="https://vimeo.com/131757759">Business Logic, a different perspective</a>
where he talks about <a href="http://udidahan.com/2009/06/07/the-fallacy-of-reuse/">the fallacy of reuse</a>.</p>

<p>Last but not least, I wrote a tiny <a href="https://github.com/jbogard/MediatR">MediatR</a> style
<a href="https://github.com/hakant/TypeScriptCommandPattern">application/command facade in TypeScript</a>. I do make use
of this module in <a href="https://github.com/hakant/HackathonPlannerAPI">HackathonPlannerAPI</a> and I&rsquo;ll write more about
it in my next post.</p>

<h2>Resist the temptation of sharing and reusing code unless you have a good justification and the right abstraction</h2>

<p>Before you decide to share code between application scenarios, think twice, think three times. If you really have
to do it, make sure you build a very clear interface around that component or module. Like Udi Dahan says in his
talk (shared above), USE this component from multiple scenarios, do not RE-USE it: If you find yourself tweaking the
component for each new scenario that&rsquo;s using it, you probably got the boundary wrong. Find the right boundary or
refactor this component back into the scenarios and stop sharing/reusing it.</p>

<p>In <a href="https://medium.com/@rdsubhas/10-modern-software-engineering-mistakes-bc67fbef4fc8#.k139s48qo">one of my favorite medium posts</a>
I&rsquo;ve read recently, the author really nails it:</p>

<blockquote><p>When Business throws more and more functionality (as expected), we sometimes react like this:
<br/><br/>
<img src="/assets/ContinuousIntegration_Part1/Shared_Logic_1.png" alt="Shared Business Logic-1" />
<br/><br/>
Instead, how should we have reacted:
<br/><br/>
<img src="/assets/ContinuousIntegration_Part1/Shared_Logic_2.png" alt="Shared Business Logic-1" />
<br/><br/></p></blockquote>

<h2>Unit or Integration&hellip; Let&rsquo;s call them all TESTING.</h2>

<p>For a good chunk of my development career, I was told that the &ldquo;unit&rdquo; in unit test is a class. So as a result,
my programming style evolved in a way that I always felt the need to design my classes with this type of testing
in mind: always being able to isolate a class from its surroundings.</p>

<p>I understand that some call this <a href="http://david.heinemeierhansson.com/2014/test-induced-design-damage.html">Test-induced design damage</a>.
Somehow in the world of C#, using Dependency Injection and programming against interfaces still feels very natural
to me. So for me, this is not a big deal and I still find it useful to be able to swap things in and out as
necessary and in every level of my implementation.</p>

<p>However, what I&rsquo;ve come to learn eventually is this: <strong>coupling your tests to your implementation details will
kill you</strong>. Just like <a href="https://vimeo.com/68375232">Ian Cooper explains in his brilliant talk at NDC Oslo</a>. So
if you&rsquo;re writing tests for each and every single one of your classes, it&rsquo;s very likely that you&rsquo;re doing it wrong
and soon you&rsquo;ll find out that your tests are slowing you down instead of giving you the feedback and agility you
were hoping for when you started.</p>

<p>Instead, <strong>find your significant boundaries</strong>. Meaningful boundaries that are composed of one or more classes and
that represent business needs. The key is this: even if your implementation details change, after let&rsquo;s say a BIG
refactoring, your tests SHOULD NOT need to change.</p>

<p>What is a better significant boundary than the whole application boundary? This is essentially what a MediatR style
pattern gives you. One narrow facade for your whole application. What a great boundary to write your tests against!</p>

<h2>Speed of tests matter but technology is changing too</h2>

<p>One big reason (heck, maybe the only reason) why people will tell you to design your system in a
way that you can swap out your database in favor of a test double (f.ex by using a repository pattern) is the speed
of tests.</p>

<p>This could be a necessary evil back in the days where databases and infrastructure was bulky and slow. But is
this still true? For <a href="https://github.com/hakant/HackathonPlannerAPI">Hackathon Planner API project</a>
I wrote 15 tests which execute all application scenarios against a <a href="https://aws.amazon.com/blogs/aws/dynamodb-local-for-desktop-development/">DynamoDB Local Emulator</a>,
so nothing is being swapped in or out on the application side but each test sets up and tears down the NOSQL
document store &ndash; so that all tests are completely isolated from each other.</p>

<p>The result is amazing. It takes ~2 seconds to run all the tests. Let&rsquo;s say if my application grows in
size in the future and that I had to execute 300 tests, it would still take me below a minute to
run all the tests!</p>

<p>I know this example will not represent every project out there in the wild for different sorts of reasons
but when it does, it definitely blurs the line between &ldquo;Service&rdquo; and &ldquo;Unit&rdquo; tests in
<a href="https://martinfowler.com/bliki/TestPyramid.html">the test pyramid</a>:</p>

<br/>


<p><img src="https://martinfowler.com/bliki/images/testPyramid/test-pyramid.png" width="400" alt="Test-Pyramid" /></p>

<br/>


<p>Thanks for reading. In Part 2 there will be code, I promise.</p>

<blockquote><p><strong>Update</strong>:
<br/><br/></p>

<p>Part 2 is now available! You can <a href="/blog/2017/03/25/setting-up-continuous-delivery-for-a-node-dot-js-rest-api-part-2/">reach it here</a>.</p></blockquote>

<p>~Hakan</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zen of Architecture]]></title>
    <link href="http://www.hakantuncer.com/blog/2015/06/28/zen-of-architecture/"/>
    <updated>2015-06-28T13:19:40+02:00</updated>
    <id>http://www.hakantuncer.com/blog/2015/06/28/zen-of-architecture</id>
    <content type="html"><![CDATA[<p><a href="/blog/2015/05/02/devintersection-and-anglebrackets-in-phoenix/">I was at DevIntersection Conference</a> in Phoenix around mid May 2015. Apart from the regular conference schedule, I took a few interesting workshops one of which was &ldquo;Zen of Architecture&rdquo; by <a href="http://www.oreilly.com/pub/au/741">Juval Lowy</a>.</p>

<p>As the name clearly implies one day workshop was all about software architecture and the ways to approach it. Juval talked about a method that he uses for decomposing systems and making design decisions.</p>

<p>It was a full day of content storm with as little breaks in between as possible (this is the Juval style I suppose). I&rsquo;ve been reviewing my notes and presentation slides thinking this can become a book if I try to write everything. So instead, this post focuses on a certain part of the workshop which I think covers the core idea.</p>

<h2>The Method</h2>

<p>Juval introduces a method which he calls &ldquo;The Method&rdquo; for decomposing a system. Achieving the right decomposition of a system is one of the most important things in software architecture. The end result of your decomposition is your architecture.</p>

<blockquote><p>For the beginner architect, there are many options<br/>
For the master architect, there are only a few</p></blockquote>

<h2>Functional Decomposition</h2>

<p>The biggest sin of any software architect. Never ever do functional decomposition!</p>

<br/>


<h4>What is it?</h4>

<ul>
<li>This is also known as the flow-chart decomposition.</li>
<li>Basing services and system components on the order of logical steps in use cases.</li>
<li>Decomposing the system based on features, functional requirements and time.</li>
</ul>


<p>Juval emotionally talks about &ldquo;functional decomposition&rdquo; easily for more than an hour. Can&rsquo;t repeat enough how bad it is and how we&rsquo;re all guilty of doing it from time to time and why we should be very conscious about resisting our bad habits. Say &ldquo;functional decomposition&rdquo; to Juval one more time and he&rsquo;ll kill you right there without blinking.</p>

<br/>


<h4>Why is it so bad?</h4>

<ul>
<li>It leads to duplicating behaviors across services.</li>
<li>It leads to explosion and bloating of services and intricate relationships inside and between them.</li>
<li>It couples multiple services to data contract.</li>
<li>Promotes implementing use cases in higher level terms thus difficult to reuse same behavior in another use case.</li>
<li>Couples services to order and current use cases.</li>
<li>Prevents single point of entry.</li>
</ul>


<br/>


<h4>Example of Functional Decomposition</h4>

<p>Here are  a few slides from the workshop that shows Functional Decomposition in action.</p>

<p><img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2021.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2023.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2024.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2025.jpg" style="border: solid;"></p>

<h2>Volatility-Based Decomposition</h2>

<p>If you would learn one thing from this entire course this should be it, Juval mentioned repeatedly. <strong>You should always decompose a system based on volatility.</strong></p>

<ul>
<li>Identify areas of potential change and encapsulate them in services.</li>
<li>Look for functional potential changes but <strong>not domain functional</strong>. Meaning that while looking for volatility, don&rsquo;t speculate on potential changes to the nature of the business. Don&rsquo;t overdo it.</li>
<li>Implement behavior as interactions between services or subsystems.</li>
<li>Create your milestones based on integration of these services not features.</li>
</ul>


<p>This is the universal principle of good design says Juval. Encapsulate change to insulate. Do not resonate with change. Functional decomposition on the other hand maximizes the impact of change because it&rsquo;s coupled to it.</p>

<br/>


<h4>Challenges with Volatility-Based Decomposition</h4>

<p>There are challenges in creating a Volatility-Based Decomposition. First of all it usually takes longer than functional because volatility is not often self evident. On the other hand features are kept thrown at your face. People around you are feature thirsty, they&rsquo;ll keep asking for their features. You should instead fight the insanity and focus on the bigger picture and volatilities. Getting the management support is usually another challenge. Juval said architects should be responsible, fight against these opposing forces and do what is right.</p>

<br/>


<h4>Axes of volatility</h4>

<p>There are two axes of volatility.</p>

<ul>
<li>At the same customer over time</li>
<li>At the same time across customers</li>
</ul>


<p>These axes should be independent from each other. So encapsulate them from each other as well. When they&rsquo;re not independent it&rsquo;s a sign of functional decomposition.</p>

<p>Prior to architecture and decomposition, as part of requirements gathering and analysis, prepare a list of areas of volatility. Ask what could change along the axes of volatility.</p>

<br/>


<h4>Example of Volatility-Based Decomposition</h4>

<p>Below, you&rsquo;ll find 5 slides from the workshop that brainstorms on possible volatilities of a trading system. Here are some guidelines for capturing volatility:</p>

<ul>
<li>The objective is to have a mindset of &ldquo;what could possibly change?&rdquo;</li>
<li>Capturing the areas of volatility earlier is better than later. The later you figure it out the more it will cost you.</li>
<li>Once settled on the ares of volatility encapsulate them in components of architecture.</li>
<li>You don&rsquo;t need an exhaustive list. This is a process of diminishing returns. Don&rsquo;t overdo it.</li>
<li>Some volatile areas may relate too much to the nature of your business. This type of volatility is out of your scope.</li>
</ul>


<p><img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2026.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2027.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2028.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2029.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2030.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2032.jpg" style="border: solid;"></p>

<br/>


<h4>Let&rsquo;s analyze this decomposition.</h4>

<p>Transition from list of areas of volatility to services is hardly ever pure 1:1. Sometimes a single service encapsulates multiple areas. Some areas may map to an operational concept or may be encapsulated in a third party component.</p>

<p>Always encapsulate the data storage volatility behind data access services. Encapsulate where the storage is, what technology is used to access it and refer your storage as storage, not as database or whatever the actual technology is.</p>

<p>Following 3 slides do further analysis of the decomposition in this example. Please refer to the diagram above as you read the key points:</p>

<p><img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2034.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2035.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2036.jpg" style="border: solid;"></p>

<h2>Decomposition and Business</h2>

<p>Avoid encapsulating changes to the nature of your business. Because</p>

<ul>
<li>you&rsquo;ll need it very rarely</li>
<li>you&rsquo;ll be diving into speculation and speculation based design trap</li>
<li>when you do it you&rsquo;ll probably do it very poorly because there are too many unknowns (and again speculations)</li>
</ul>


<p>While designing a system for your business don&rsquo;t only focus on your own also keep your competitor in mind. Design both for you and your competitor. This is a useful posture in designing systems and it&rsquo;s not about features or functionality but it&rsquo;s about understanding the nature of the business. Keeping your competitors in mind and not only focusing on your own business will help you understand the nature of the business even better. This way you can have a better judgement about what is volatile and what is not.</p>

<h2>Decomposition and Longevity</h2>

<p>Volatility is very closely tied to longevity. The longer things do not change the longer they have till they do change or are replaced. The more frequently things change the more likely they would change in the future.</p>

<p>You must take into account impact from change regardless of your requirements. Ask yourself what has changed over the past 5-7 years and what will change in the next 5-7 years. Encapsulate things that would change within the life of your system.</p>

<h2>Further topics</h2>

<p>For the rest of the day Juval dived into</p>

<ul>
<li>layered architectures</li>
<li>typical layers</li>
<li>definition of managers, engines, resource access services and differences between them</li>
<li>open and closed architectures</li>
<li>what not to do when using &ldquo;the method&rdquo;</li>
<li>creating call graphs within system components and observing/revealing anti-patterns</li>
<li>a bonus section called &ldquo;what about agile?&rdquo;</li>
</ul>


<p>All in all it was a mind tickling workshop and I hope I could give you more than only a taste. I may write other posts about the bullet points above if/when I find the time.</p>

<p>Thanks for reading</p>

<p>Hakan</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Playing Around With Redis & SignalR]]></title>
    <link href="http://www.hakantuncer.com/blog/2014/10/26/playing-around-with-redis-and-signalr-part-1/"/>
    <updated>2014-10-26T19:32:27+01:00</updated>
    <id>http://www.hakantuncer.com/blog/2014/10/26/playing-around-with-redis-and-signalr-part-1</id>
    <content type="html"><![CDATA[<p>A good friend of mine contacted me a couple of weeks ago and asked if I would accept a freelancing project. Around research and prototyping. Possibly leading to a new architecture for their existing product. The existing architecture didn&rsquo;t allow any further room for the new requirements they had &ndash; he told me during our skype conversation. I thought it was fun and took the offer. R&amp;D and freelancing.. not bad huh!</p>

<p>So as part of the assignment I&rsquo;ve been doing some interesting reasearch and prototyping so why not blog about it as well I thought.</p>

<h2>Going technical:</h2>

<p>One of the requirements is to decouple the system components so that they can be deployed and run on different boxes. So a typical &ldquo;scaling out&rdquo; problem.</p>

<p>The other requirement is to enable these components to talk to each other about the changes happening through-out the system. So these components have to be able to send/receive messages to/from each other. This is especially important after separating these components obviously.</p>

<p>After studying the existing architecture and reading through some code, this was my initial design. I&rsquo;ve changed the application names to something more generic to keep it confidential but the technical idea behind it is still the same:</p>

<p><img src="/assets/Playing_Around_With_Redis/SystemArchitectureDiagram.png" alt="System Architecture Diagram" /></p>

<p>Let&rsquo;s look at some of the key components:</p>

<ul>
<li><strong>App Server</strong>: The application server is running a mission critical service that needs to be very fast. It is actually orchestrating incoming and outgoing voice calls, can&rsquo;t afford to be slow. Obviously there is also some bookkeeping that the application server does. The states of the calls and users change over time and this kind of application state information is also interesting to the other parts of the system for various reasons like monitoring, dashboards or some other type of actions that needs to be taken.</li>
<li><strong>Web UI</strong>: This is basically where customers will sign in and manage their system and data. They can do administrative work as well as look at various dashboards to see what is going on. This is going to become a self-hosted (yes OWIN) MVC 5 application.</li>
<li><strong>Web API</strong>: Most things that can be done with the Web UI should be done with API as well. So that customers can be creative and build their own solutions &amp; extensions. This will be built with Web API 2.0 and self hosted.</li>
</ul>


<p>I will tell more about the other components in the future posts. But careful readers should have already noticed that some of the requirements beg for a &ldquo;message broker&rdquo; / &ldquo;service bus&rdquo; type of a system in between to coordinate messaging.
Let&rsquo;s talk a bit about <a href="http://redis.io/" target="_blank">Redis</a> now.</p>

<h2>Redis:</h2>

<p>Here is the official definiton of Redis:</p>

<blockquote><p>Redis is an open source, BSD licensed, advanced key-value cache and store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets, sorted sets, bitmaps and hyperloglogs.</p></blockquote>

<p>At <a href="http://www.niposoftware.com/" target="_blank">NIPO Software</a> we&rsquo;ve been using Redis for a while but frankly I didn&rsquo;t know that Redis was also supporting pub/sub messaging. As a solution the first thing that came to my mind was <a href="http://www.rabbitmq.com/" target="_blank">RabbitMQ</a>, which is a message broker. But Redis can do other interesting things as well. So I focused on Redis instead.</p>

<p>Here is what Wikipedia says about the &ldquo;Publish–subscribe pattern&rdquo;:</p>

<blockquote><p>In software architecture, publish–subscribe is a messaging pattern where senders of messages, called publishers, do not program the messages to be sent directly to specific receivers, called subscribers. Instead, published messages are characterized into classes, without knowledge of what, if any, subscribers there may be. Similarly, subscribers express interest in one or more classes, and only receive messages that are of interest, without knowledge of what, if any, publishers there are.</p></blockquote>

<p>And what Redis adds to it:</p>

<blockquote><p>This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology.</p></blockquote>

<p>Redis is also a cache store and a persistent storage. If you tell Redis to do so, it will take occasional snapshots of the data in it&rsquo;s memory and persist it to disk. As you progress through the Redis documentation it feels like Redis is all you need : ) F.ex here is where Redis snapshotting feature is configured:</p>

<p>```bash</p>

<h6>########################## SNAPSHOTTING</h6>

<p>#</p>

<h1>Save the DB on disk:</h1>

<p>#</p>

<h1>save <seconds> <changes></h1>

<p>#</p>

<h1>Will save the DB if both the given number of seconds and the given</h1>

<h1>number of write operations against the DB occurred.</h1>

<p>#</p>

<h1>In the example below the behaviour will be to save:</h1>

<h1>after 900 sec (15 min) if at least 1 key changed</h1>

<h1>after 300 sec (5 min) if at least 10 keys changed</h1>

<h1>after 60 sec if at least 10000 keys changed</h1>

<p>#</p>

<h1>Note: you can disable saving at all commenting all the &ldquo;save&rdquo; lines.</h1>

<p>#</p>

<h1>It is also possible to remove all the previously configured save</h1>

<h1>points by adding a save directive with a single empty string argument</h1>

<h1>like in the following example:</h1>

<p>#</p>

<h1>save &ldquo;&rdquo;</h1>

<h1>save 900 1</h1>

<h1>save 300 10</h1>

<h1>save 60 10000</h1>

<p>```</p>

<p>Depending on how critical your data is you can specify different levels of aggressiveness. As with everything the more aggressively you save the more stress you put on the system. On the other hand you can disable saving completely which will make Redis a purely caching server.</p>

<p>If you want to quickly get up and running with Redis I recommend the <a href="http://www.pluralsight.com/courses/building-nosql-apps-redis" target="_blank">Pluralsight course from John Sonmez</a>. It does a great job on getting you from zero to sixty.</p>

<h2>Redis on Windows</h2>

<p>Redis is an open source project and only supports Linux. But Microsoft Open Tech group develops and supports a <a href="https://github.com/MSOpenTech/redis" target="_blank">Windows port targeting Win64</a>. On the other hand I haven&rsquo;t seen a single source that recommends running Redis Windows in production even though Microsoft claims that it&rsquo;s heavily tested and production ready. General sentiment is like &ldquo;why would you do that when you can just run it on a Linux VM&rdquo; which I can understand for SAAS scenarios.</p>

<p>For the project that I&rsquo;m working on there is also installation scenarios on customer premises so asking them to get a Linux machine is a bit over the top my friend says. So we&rsquo;re willing to give the Microsoft Redis a chance. Hey these guys are also running <a href="http://azure.microsoft.com/en-us/services/cache/" target="_blank">Redis on Azure</a> after all&hellip;</p>

<p><a href="https://servicestack.net/" target="_blank">ServiceStack</a> is one of the frameworks that put focus on Redis. Supporting Redis is one of their selling points:</p>

<p><img src="/assets/Playing_Around_With_Redis/ServiceStack.png" alt="ServiceStack products" /></p>

<p>ServiceStack also distributes <a href="https://github.com/ServiceStack/redis-windows" target="_blank">MS Open Tech Redis port of Windows on GitHub</a>. If you want to run the Windows version of Redis there are very useful information here on how to install Redis Windows as a service or how to use <a href="http://docs-v1.vagrantup.com/v1/docs/getting-started/index.html" target="_blank">Vagrant</a> to install the Linux version of Redis on a Windows box with virtualization through <a href="https://www.virtualbox.org/" target="_blank">VirtualBox</a>.</p>

<h2>Redis C# Clients</h2>

<p>There are <a href="http://redis.io/clients" target="_blank">a bunch of clients</a> out there implemented for Redis with various languages. As of this writing this is how the list looks like for C#. The ones that are starred are the recommended clients by Redis.</p>

<p><img src="/assets/Playing_Around_With_Redis/RedisCSharpClients.png" alt="Redis C# Clients" /></p>

<p>I&rsquo;ve picked the ServiceStack client over StackExchange because of the richer and nicer to use library. It looks like the StackExchange client is claiming to be the fastest though.</p>

<p>Right after starting to use the latest ServiceStack Redis client I&rsquo;ve noticed that they&rsquo;ve switched to a commercial license with version 4. But <a href="https://github.com/ServiceStackV3/ServiceStackV3" target="_blank">the previous version 3</a> continues to be BSD license. So I fell back to this version after finding that out. Version 3 is also a pretty complete library and for my current purposes it doesn&rsquo;t really matter.</p>

<p>Here is how the API looks like (click <a href="http://mono.servicestack.net/img/Redis-annotated.png" target="_blank">here</a> for a version that you can zoom):</p>

<p><img src="/assets/Playing_Around_With_Redis/Redis-annotated.png" alt="Redis Client API Overview" /></p>

<h2>A User Scenario</h2>

<p>One of the scenarios that can be implemented with this prototype is as follows (remember the architecture diagram from the beginning of this post):</p>

<ul>
<li>When a user adds a record to the system through the Web UI (management site) the App Server needs to know about that record without going to the database.</li>
<li>When something interesting happens on the App Server the Web UI needs to be notified and then it should refresh the information on the connected clients' browsers. A typical dashboard scenario which should not require manual page refresh.</li>
</ul>


<p>Thanks for reading.</p>

<p>Hakan</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Disaster Recovery on Azure]]></title>
    <link href="http://www.hakantuncer.com/blog/2014/04/27/disaster-recovery-on-azure/"/>
    <updated>2014-04-27T19:30:37+02:00</updated>
    <id>http://www.hakantuncer.com/blog/2014/04/27/disaster-recovery-on-azure</id>
    <content type="html"><![CDATA[<br/><br/>


<blockquote><h4>UPDATE:</h4>

<p>Things keep changing &amp; improving really FAST on Azure. This post may not reflect the latest capabilities in terms of Disaster Recovery for Azure Applications. Make sure to read the subject on <a href="https://msdn.microsoft.com/en-us/library/azure/dn251004.aspx">MSDN</a> as well.</p></blockquote>

<p>There are things we tend to ignore. Things that we don&rsquo;t want to spend time on or think about because there are no immediate benefits. Disaster recovery is one of those topics. The chances are fairly low, but the impact on our business is catastrophic. The consequences can go as far as losing the business completely. Unless there is some sort of a disaster recovery plan in place.</p>

<p>Because I&rsquo;m working with Microsoft Azure on a daily basis as part of my job, I&rsquo;ll be focusing on Azure but the fundamental principles and concepts should be universal for every cloud platform, or even for on premises type scenarios.</p>

<h2>Two key objectives</h2>

<br>


<blockquote><h5>Recovery Time Objective (RTO)</h5>

<p>Recovery Time Objective is the maximum amount of time allocated for restoring application functionality.</p></blockquote>

<p>This is usually a requirement coming from the business. Basically the question is how critical is your platform and how much down time can you tolerate in such a catastrophic event?</p>

<blockquote><h5>Recovery Point Objective (RPO)</h5>

<p>The recovery point objective (RPO) is the acceptable time window of lost data due to the recovery process.</p></blockquote>

<p>For example, if the RPO is one hour, you must completely back up or replicate the data at least every hour. Once you bring up the application in an alternate datacenter, the backup data may be missing up to an hour of data. Like RTO, critical applications target a much smaller RPO.</p>

<p>These two key objectives determine the approach that needs to be followed and therefore the effort and cost of the whole disaster recovery plan.</p>

<p>The purpose of this blog post is not to describe solutions for each possible combination of these two objectives, but to describe a couple of approaches that can tolerate relatively long recovery time objective (~24 hours) with a more aggressive (so short) recovery point objective.</p>

<p>Don&rsquo;t forget that having a short RTO and RPO together can be both costly and complex to implement depending on the storage needs of your applications.</p>

<h2>Running business on a single datacenter</h2>

<p>Let&rsquo;s say you haven&rsquo;t thought on a disaster recovery plan yet. If your software is deployed on a single datacenter and making use of Azure Storage and/or Azure SQL Database you are at risk of losing your business should your datacenter goes dark due to a disaster or a catastrophic failure.</p>

<p>An Azure datacenter is equipped with fault domains and redundancy to keep your service highly available but these are all inside the datacenter. If the whole datacenter goes down all the compute instances, databases and storage services will go down with it.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Single_Region_Deployment.png" width="500" height="438" title="&lsquo;Single Region Deployment&rsquo;" ></p>

<p>Assuming that you still have your software in-house somewhere, there should be no risk of losing the compute instances forever. By creating and deploying the cloud packages on another datacenter the compute instances can be recovered. <strong>The key to business continuity is to be able to recover the data that is stored on Azure Storage and Azure SQL Databases.</strong></p>

<h2>Frequently backup data outside the datacenter</h2>

<p>Whatever the recovery objectives are, backing up application data outside of the datacenter is a must for business continuity. How frequently the data is backed up or synced outside the datacenter will determine the recovery point objective.</p>

<p>When there are backups available outside the datacenter, the environment can be moved to another datacenter by restoring the data and installing compute instances using existing cloud service packages.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Redeploy_Azure_Datacenter.png" alt="Redeploy to another datacenter" /></p>

<p>This is called &ldquo;redeploy&rdquo; recovery model and as you might already guess has a long recovery time objective. All the individual pieces of the environment needs to be moved to another datacenter and redeployed.</p>

<h2>How to backup Azure data (together with RPO and RTO considerations)</h2>

<br><br>


<h4>1. Azure Storage</h4>

<p>The good news is that Azure Storage Service has built-in replication strategies. Two of them are geographical redundancy, meaning that all storage data is replicated across datacenters. That&rsquo;s exactly what disaster recovery is about.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Azure_Storage_Redundancy.png" alt="Azure Storage Geo Redundancy" /></p>

<p>The difference between &ldquo;Geo Redundant&rdquo; and &ldquo;Read-Access Geo Redundant&rdquo; is that the latter allows the redundant data to be accessed at all times in a read-only fashion. This brings us to another important point:</p>

<blockquote><p>Here is what Microsoft&rsquo;s documentation say about the estimated azure storage failover time in case of a disaster:  &ldquo;estimated time that the data will be accessible to customers after a disaster is 24 hours.&rdquo;</p></blockquote>

<p>In the light of these information 3 options appear for backing up the Azure Storage:</p>

<ul>
<li><strong>Geo-Redundancy</strong>: There is no SLA but RPO is practically very short. RTO is long. Only after about 24 hours the failover process will be completed by Microsoft and the data will be available again.</li>
<li><strong>Read-Access Geo Redundant</strong>: RPO is again the same as above. However almost immediately the redundant data can be accessed (read-only) which allows systems to run in a degraded mode (if they&rsquo;re designed in such a fault tolerant way of course). Then again in 24 hours everything will be fully operational.</li>
<li><strong>Custom</strong>: If RTO needs to be shorter, the only option is to look for a third party product that can replicate an azure storage to another datacenter. This redundant storage will always be available in Read/Write mode (since it&rsquo;s just another regular storage account, only in another geographical region).</li>
</ul>


<br>


<h4>2. Azure SQL Databases</h4>

<p>Yesterday I woke up to an <a href="http://view.email.microsoftemail.com/?j=fe9016787161057a71&amp;m=fe621570756503797d1c&amp;ls=fe1817787c60027e711d79&amp;l=fec21c767365017e&amp;s=fe2212717465037a701d78&amp;jb=ff5e177873&amp;ju=fe5711777060017b7611">annoucement from Microsoft</a> introducing new types of databases in Azure with new disaster recovery features:</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/DisasterRecoveryPerDatabaseType.png" alt="Disaster Recovery Per Database Type" /></p>

<p>Let&rsquo;s look at each option and see what it means:</p>

<ul>
<li><strong>Restore to an alternate Azure region</strong>: This phrase means &ldquo;no silver bullet&rdquo;. Basic type database owners are responsible with their own backup and restore operations. Luckily there is a convenient way to backup and restore an Azure SQL Database regardless of its type.</li>
</ul>


<br>


<h5>Automatic Database Export:</h5>

<p>Actually there are a couple of ways to replicate or backup a database. But most of these options have their own shortcomings for disaster recovery. There is a feature called <a href="http://msdn.microsoft.com/en-US/library/azure/ff951624.aspx">Database Copy</a> which creates a <a href="http://technet.microsoft.com/en-us/library/ms151176.aspx">transactionally consistent replica</a> of the source database in <strong>the same datacenter.</strong> Because the replica resides in the same datacenter there is no geo-redundancy. But after the copying is completed, this database can be exported to a storage account in another datacenter.</p>

<p>This is exactly what <strong>Automatic Database Export</strong> feature does. It first replicates the database with a copy operation, thus getting a transactionally consistent copy of the database, then exports it to the storage account that is configured. To see how it is configured you can visit <a href="http://blogs.msdn.com/b/sql-bi-sap-cloud-crm_all_in_one_place/archive/2013/07/24/sql-azure-automated-database-export.aspx">this blog post</a>.</p>

<p>A direct manual export operation itself does not generate a transactionally consistent copy of a database. This means you may end up having an Order item in the database with a missing OrderDetails. Automatic Database Export however takes care of this problem. The documentation was not very clear on that so I asked the man himself:</p>

<br>




<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/hakant">@hakant</a> <a href="https://twitter.com/Azure">@Azure</a> yes I&#39;m pretty sure it is</p>&mdash; Scott Guthrie (@scottgu) <a href="https://twitter.com/scottgu/statuses/460180886915276801">April 26, 2014</a></blockquote>


<script async src="http://www.hakantuncer.com//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>So we&rsquo;re on the right track here.</p>

<br>


<blockquote><p><strong>Attention:</strong> First part of Automatic Database Export is the replication of the database with Database COPY operation which runs two databases at the same time. This means export operations are partially doubling the database costs.</p></blockquote>

<p>So far so good&hellip; But what determines RTO and RPO for Automatic Database Export? This analysis depends on the frequency and the destination storage configured for the database export operation.</p>

<br>


<ul>
<li><em>Exporting to a separate storage account on another datacenter:</em></li>
</ul>


<p>In this scenario, the RPO will simply be around [Database Export Frequency]. If the database is exported every 6 hours, in the worst case scenario 6 hours of data may be lost.</p>

<p>RTO is simply the [Database Restore Duration]. No need to wait for Microsoft to execute the Azure Storage failover process. The database export file is immediately available on the storage account in the other datacenter.</p>

<br>


<ul>
<li><em>Exporting to the primary storage account on the same datacenter:</em></li>
</ul>


<p>In this scenario, we&rsquo;re relying on the Automated Azure Storage Geo-Replication to get the exported database transferred to another datacenter. Since this scenario uses the geo-replication feature of the storage account there won&rsquo;t be any additional costs for geo-reduntant storage. This is nice indeed.</p>

<p>But in the worst case scenario the RPO can go as far as [2 * Database Export Frequency] if the datacenter happens to fail during the geo-replication process. In that case, the last export file won&rsquo;t be available after the failure but only the one before that will be.</p>

<p>On the other hand, the RTO is [Estimated Azure Storage Geo-Failover Time + Database Restore Duration]. Again, Microsoft&rsquo;s estimation for Azure Storage geo-failover is around 24 hours.</p>

<p>So the moral of the story is that having a dedicated separate Azure Storage account on the other datacenter is beneficial for both RPO and RTO. But as usual the downside is financial. The separate storage account is also billed separately. Both for storage and bandwith to transfer the data. Though it&rsquo;s also worth mentioning that Azure Storage is <a href="http://azure.microsoft.com/en-us/pricing/details/storage/">fairly cheap</a>.</p>

<p>Now let&rsquo;s move to the other disaster recovery features.</p>

<ul>
<li><p><strong>Geo-Replication Passive Replica</strong>: Unfortunately there is no documentation available for this mysterious feature yet. Frankly there is no single mention of this feature anywhere else at the time of this writing. So the technique described above for the Basic databases also applies here until Microsoft actually ships this feature or shares more about it. I&rsquo;ll update this section when that happens.</p></li>
<li><p><strong>Active Geo-Replication</strong>: This one is a silver bullet solution. Unfortunately only the Premium database owners can make use of this feature. With Active Geo-Replication, you can create and maintain up to four readable secondary databases across geographic regions. Basically this gives a very short RPO and RTO for the price of running multiple Premium databases (<a href="http://azure.microsoft.com/en-us/pricing/details/sql-database/#basic-standard-and-premium">which is quite a lot</a>). You can read more about <a href="http://msdn.microsoft.com/en-US/library/azure/dn741339.aspx">Active Geo-Replication on MSDN</a>.</p></li>
</ul>


<h2>Final thoughts</h2>

<p>If your business can tolerate larger than ~24 hrs recovery time objective (RTO) Azure provides a couple of inexpensive features that you can already start making use of. These features also require very little effort to setup and you guarantee business continuity in case of a catastrophic event.</p>

<p>The most cost effective strategy is the following:</p>

<ol>
<li>Make sure your storage account is configured for Geo-Redundancy (default).</li>
<li>Turn on Automatic Database Export and configure a high frequency that makes sense. Keep track of how long this operation takes and adjust your frequency based on that as well.</li>
<li>Set your default azure storage account as the destination of automatic database export. So rely on the geo-redundancy of your storage account.</li>
</ol>


<p>If you need a shorter RTO build up on this strategy. This post is mainly focused on &ldquo;Redeploy&rdquo; pattern. There are other patterns available on MSDN and some are focused on more aggressive RTO&rsquo;s. I recommend reading: <a href="http://msdn.microsoft.com/en-us/library/dn251004.aspx">Disaster Recovery and High Availability for Azure Applications</a>.</p>

<h2>References</h2>

<p>Most information on this blog post is obtained from the following MSDN pages. Some others are direct observations &amp; usage from Azure Management Portal.</p>

<ul>
<li><a href="http://msdn.microsoft.com/en-us/library/dn251004.aspx">Disaster Recovery and High Availability for Azure Applications</a></li>
<li><a href="http://msdn.microsoft.com/library/azure/hh852669.aspx">Azure SQL Database Business Continuity</a></li>
<li><a href="http://msdn.microsoft.com/en-US/library/azure/ff951624.aspx">Copying Databases in Azure SQL Database</a></li>
<li><a href="http://msdn.microsoft.com/en-US/library/azure/hh335292.aspx">How to: Import and Export a Database (Azure SQL Database)</a></li>
<li><a href="http://blogs.msdn.com/b/windowsazure/archive/2014/04/04/sql-database-updates-coming-soon-to-the-premium-preview.aspx">SQL Database updates coming soon to the Premium preview</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
