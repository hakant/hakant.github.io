<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Architecture | Hakan Tuncer's Blog]]></title>
  <link href="http://www.hakantuncer.com/blog/categories/architecture/atom.xml" rel="self"/>
  <link href="http://www.hakantuncer.com/"/>
  <updated>2016-01-19T20:34:26+01:00</updated>
  <id>http://www.hakantuncer.com/</id>
  <author>
    <name><![CDATA[Hakan Tuncer]]></name>
    <email><![CDATA[hakantuncer@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Zen of Architecture]]></title>
    <link href="http://www.hakantuncer.com/blog/2015/06/28/zen-of-architecture/"/>
    <updated>2015-06-28T13:19:40+02:00</updated>
    <id>http://www.hakantuncer.com/blog/2015/06/28/zen-of-architecture</id>
    <content type="html"><![CDATA[<p><a href="/blog/2015/05/02/devintersection-and-anglebrackets-in-phoenix/">I was at DevIntersection Conference</a> in Phoenix around mid May 2015. Apart from the regular conference schedule, I took a few interesting workshops one of which was &ldquo;Zen of Architecture&rdquo; by <a href="http://www.oreilly.com/pub/au/741">Juval Lowy</a>.</p>

<p>As the name clearly implies one day workshop was all about software architecture and the ways to approach it. Juval talked about a method that he uses for decomposing systems and making design decisions.</p>

<p>It was a full day of content storm with as little breaks in between as possible (this is the Juval style I suppose). I&rsquo;ve been reviewing my notes and presentation slides thinking this can become a book if I try to write everything. So instead, this post focuses on a certain part of the workshop which I think covers the core idea.</p>

<h2>The Method</h2>

<p>Juval introduces a method which he calls &ldquo;The Method&rdquo; for decomposing a system. Achieving the right decomposition of a system is one of the most important things in software architecture. The end result of your decomposition is your architecture.</p>

<blockquote><p>For the beginner architect, there are many options<br/>
For the master architect, there are only a few</p></blockquote>

<h2>Functional Decomposition</h2>

<p>The biggest sin of any software architect. Never ever do functional decomposition!</p>

<br/>


<h4>What is it?</h4>

<ul>
<li>This is also known as the flow-chart decomposition.</li>
<li>Basing services and system components on the order of logical steps in use cases.</li>
<li>Decomposing the system based on features, functional requirements and time.</li>
</ul>


<p>Juval emotionally talks about &ldquo;functional decomposition&rdquo; easily for more than an hour. Can&rsquo;t repeat enough how bad it is and how we&rsquo;re all guilty of doing it from time to time and why we should be very conscious about resisting our bad habits. Say &ldquo;functional decomposition&rdquo; to Juval one more time and he&rsquo;ll kill you right there without blinking.</p>

<br/>


<h4>Why is it so bad?</h4>

<ul>
<li>It leads to duplicating behaviors across services.</li>
<li>It leads to explosion and bloating of services and intricate relationships inside and between them.</li>
<li>It couples multiple services to data contract.</li>
<li>Promotes implementing use cases in higher level terms thus difficult to reuse same behavior in another use case.</li>
<li>Couples services to order and current use cases.</li>
<li>Prevents single point of entry.</li>
</ul>


<br/>


<h4>Example of Functional Decomposition</h4>

<p>Here are  a few slides from the workshop that shows Functional Decomposition in action.</p>

<p><img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2021.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2023.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2024.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2025.jpg" style="border: solid;"></p>

<h2>Volatility-Based Decomposition</h2>

<p>If you would learn one thing from this entire course this should be it, Juval mentioned repeatedly. <strong>You should always decompose a system based on volatility.</strong></p>

<ul>
<li>Identify areas of potential change and encapsulate them in services.</li>
<li>Look for functional potential changes but <strong>not domain functional</strong>. Meaning that while looking for volatility, don&rsquo;t speculate on potential changes to the nature of the business. Don&rsquo;t overdo it.</li>
<li>Implement behavior as interactions between services or subsystems.</li>
<li>Create your milestones based on integration of these services not features.</li>
</ul>


<p>This is the universal principle of good design says Juval. Encapsulate change to insulate. Do not resonate with change. Functional decomposition on the other hand maximizes the impact of change because it&rsquo;s coupled to it.</p>

<br/>


<h4>Challenges with Volatility-Based Decomposition</h4>

<p>There are challenges in creating a Volatility-Based Decomposition. First of all it usually takes longer than functional because volatility is not often self evident. On the other hand features are kept thrown at your face. People around you are feature thirsty, they&rsquo;ll keep asking for their features. You should instead fight the insanity and focus on the bigger picture and volatilities. Getting the management support is usually another challenge. Juval said architects should be responsible, fight against these opposing forces and do what is right.</p>

<br/>


<h4>Axes of volatility</h4>

<p>There are two axes of volatility.</p>

<ul>
<li>At the same customer over time</li>
<li>At the same time across customers</li>
</ul>


<p>These axes should be independent from each other. So encapsulate them from each other as well. When they&rsquo;re not independent it&rsquo;s a sign of functional decomposition.</p>

<p>Prior to architecture and decomposition, as part of requirements gathering and analysis, prepare a list of areas of volatility. Ask what could change along the axes of volatility.</p>

<br/>


<h4>Example of Volatility-Based Decomposition</h4>

<p>Below, you&rsquo;ll find 5 slides from the workshop that brainstorms on possible volatilities of a trading system. Here are some guidelines for capturing volatility:</p>

<ul>
<li>The objective is to have a mindset of &ldquo;what could possibly change?&rdquo;</li>
<li>Capturing the areas of volatility earlier is better than later. The later you figure it out the more it will cost you.</li>
<li>Once settled on the ares of volatility encapsulate them in components of architecture.</li>
<li>You don&rsquo;t need an exhaustive list. This is a process of diminishing returns. Don&rsquo;t overdo it.</li>
<li>Some volatile areas may relate too much to the nature of your business. This type of volatility is out of your scope.</li>
</ul>


<p><img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2026.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2027.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2028.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2029.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2030.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2032.jpg" style="border: solid;"></p>

<br/>


<h4>Let&rsquo;s analyze this decomposition.</h4>

<p>Transition from list of areas of volatility to services is hardly ever pure 1:1. Sometimes a single service encapsulates multiple areas. Some areas may map to an operational concept or may be encapsulated in a third party component.</p>

<p>Always encapsulate the data storage volatility behind data access services. Encapsulate where the storage is, what technology is used to access it and refer your storage as storage, not as database or whatever the actual technology is.</p>

<p>Following 3 slides do further analysis of the decomposition in this example. Please refer to the diagram above as you read the key points:</p>

<p><img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2034.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2035.jpg" style="border: solid;">
<img src="http://www.hakantuncer.com/assets/Zen_of_Architecture/IMG_2036.jpg" style="border: solid;"></p>

<h2>Decomposition and Business</h2>

<p>Avoid encapsulating changes to the nature of your business. Because</p>

<ul>
<li>you&rsquo;ll need it very rarely</li>
<li>you&rsquo;ll be diving into speculation and speculation based design trap</li>
<li>when you do it you&rsquo;ll probably do it very poorly because there are too many unknowns (and again speculations)</li>
</ul>


<p>While designing a system for your business don&rsquo;t only focus on your own also keep your competitor in mind. Design both for you and your competitor. This is a useful posture in designing systems and it&rsquo;s not about features or functionality but it&rsquo;s about understanding the nature of the business. Keeping your competitors in mind and not only focusing on your own business will help you understand the nature of the business even better. This way you can have a better judgement about what is volatile and what is not.</p>

<h2>Decomposition and Longevity</h2>

<p>Volatility is very closely tied to longevity. The longer things do not change the longer they have till they do change or are replaced. The more frequently things change the more likely they would change in the future.</p>

<p>You must take into account impact from change regardless of your requirements. Ask yourself what has changed over the past 5-7 years and what will change in the next 5-7 years. Encapsulate things that would change within the life of your system.</p>

<h2>Further topics</h2>

<p>For the rest of the day Juval dived into</p>

<ul>
<li>layered architectures</li>
<li>typical layers</li>
<li>definition of managers, engines, resource access services and differences between them</li>
<li>open and closed architectures</li>
<li>what not to do when using &ldquo;the method&rdquo;</li>
<li>creating call graphs within system components and observing/revealing anti-patterns</li>
<li>a bonus section called &ldquo;what about agile?&rdquo;</li>
</ul>


<p>All in all it was a mind tickling workshop and I hope I could give you more than only a taste. I may write other posts about the bullet points above if/when I find the time.</p>

<p>Thanks for reading</p>

<p>Hakan</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Playing Around With Redis & SignalR]]></title>
    <link href="http://www.hakantuncer.com/blog/2014/10/26/playing-around-with-redis-and-signalr-part-1/"/>
    <updated>2014-10-26T19:32:27+01:00</updated>
    <id>http://www.hakantuncer.com/blog/2014/10/26/playing-around-with-redis-and-signalr-part-1</id>
    <content type="html"><![CDATA[<p>A good friend of mine contacted me a couple of weeks ago and asked if I would accept a freelancing project. Around research and prototyping. Possibly leading to a new architecture for their existing product. The existing architecture didn&rsquo;t allow any further room for the new requirements they had &ndash; he told me during our skype conversation. I thought it was fun and took the offer. R&amp;D and freelancing.. not bad huh!</p>

<p>So as part of the assignment I&rsquo;ve been doing some interesting reasearch and prototyping so why not blog about it as well I thought.</p>

<h2>Going technical:</h2>

<p>One of the requirements is to decouple the system components so that they can be deployed and run on different boxes. So a typical &ldquo;scaling out&rdquo; problem.</p>

<p>The other requirement is to enable these components to talk to each other about the changes happening through-out the system. So these components have to be able to send/receive messages to/from each other. This is especially important after separating these components obviously.</p>

<p>After studying the existing architecture and reading through some code, this was my initial design. I&rsquo;ve changed the application names to something more generic to keep it confidential but the technical idea behind it is still the same:</p>

<p><img src="/assets/Playing_Around_With_Redis/SystemArchitectureDiagram.png" alt="System Architecture Diagram" /></p>

<p>Let&rsquo;s look at some of the key components:</p>

<ul>
<li><strong>App Server</strong>: The application server is running a mission critical service that needs to be very fast. It is actually orchestrating incoming and outgoing voice calls, can&rsquo;t afford to be slow. Obviously there is also some bookkeeping that the application server does. The states of the calls and users change over time and this kind of application state information is also interesting to the other parts of the system for various reasons like monitoring, dashboards or some other type of actions that needs to be taken.</li>
<li><strong>Web UI</strong>: This is basically where customers will sign in and manage their system and data. They can do administrative work as well as look at various dashboards to see what is going on. This is going to become a self-hosted (yes OWIN) MVC 5 application.</li>
<li><strong>Web API</strong>: Most things that can be done with the Web UI should be done with API as well. So that customers can be creative and build their own solutions &amp; extensions. This will be built with Web API 2.0 and self hosted.</li>
</ul>


<p>I will tell more about the other components in the future posts. But careful readers should have already noticed that some of the requirements beg for a &ldquo;message broker&rdquo; / &ldquo;service bus&rdquo; type of a system in between to coordinate messaging.
Let&rsquo;s talk a bit about <a href="http://redis.io/" target="_blank">Redis</a> now.</p>

<h2>Redis:</h2>

<p>Here is the official definiton of Redis:</p>

<blockquote><p>Redis is an open source, BSD licensed, advanced key-value cache and store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets, sorted sets, bitmaps and hyperloglogs.</p></blockquote>

<p>At <a href="http://www.niposoftware.com/" target="_blank">NIPO Software</a> we&rsquo;ve been using Redis for a while but frankly I didn&rsquo;t know that Redis was also supporting pub/sub messaging. As a solution the first thing that came to my mind was <a href="http://www.rabbitmq.com/" target="_blank">RabbitMQ</a>, which is a message broker. But Redis can do other interesting things as well. So I focused on Redis instead.</p>

<p>Here is what Wikipedia says about the &ldquo;Publish–subscribe pattern&rdquo;:</p>

<blockquote><p>In software architecture, publish–subscribe is a messaging pattern where senders of messages, called publishers, do not program the messages to be sent directly to specific receivers, called subscribers. Instead, published messages are characterized into classes, without knowledge of what, if any, subscribers there may be. Similarly, subscribers express interest in one or more classes, and only receive messages that are of interest, without knowledge of what, if any, publishers there are.</p></blockquote>

<p>And what Redis adds to it:</p>

<blockquote><p>This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology.</p></blockquote>

<p>Redis is also a cache store and a persistent storage. If you tell Redis to do so, it will take occasional snapshots of the data in it&rsquo;s memory and persist it to disk. As you progress through the Redis documentation it feels like Redis is all you need : ) F.ex here is where Redis snapshotting feature is configured:</p>

<p>```bash</p>

<h6>########################## SNAPSHOTTING</h6>

<p>#</p>

<h1>Save the DB on disk:</h1>

<p>#</p>

<h1>save <seconds> <changes></h1>

<p>#</p>

<h1>Will save the DB if both the given number of seconds and the given</h1>

<h1>number of write operations against the DB occurred.</h1>

<p>#</p>

<h1>In the example below the behaviour will be to save:</h1>

<h1>after 900 sec (15 min) if at least 1 key changed</h1>

<h1>after 300 sec (5 min) if at least 10 keys changed</h1>

<h1>after 60 sec if at least 10000 keys changed</h1>

<p>#</p>

<h1>Note: you can disable saving at all commenting all the &ldquo;save&rdquo; lines.</h1>

<p>#</p>

<h1>It is also possible to remove all the previously configured save</h1>

<h1>points by adding a save directive with a single empty string argument</h1>

<h1>like in the following example:</h1>

<p>#</p>

<h1>save &ldquo;&rdquo;</h1>

<h1>save 900 1</h1>

<h1>save 300 10</h1>

<h1>save 60 10000</h1>

<p>```</p>

<p>Depending on how critical your data is you can specify different levels of aggressiveness. As with everything the more aggressively you save the more stress you put on the system. On the other hand you can disable saving completely which will make Redis a purely caching server.</p>

<p>If you want to quickly get up and running with Redis I recommend the <a href="http://www.pluralsight.com/courses/building-nosql-apps-redis" target="_blank">Pluralsight course from John Sonmez</a>. It does a great job on getting you from zero to sixty.</p>

<h2>Redis on Windows</h2>

<p>Redis is an open source project and only supports Linux. But Microsoft Open Tech group develops and supports a <a href="https://github.com/MSOpenTech/redis" target="_blank">Windows port targeting Win64</a>. On the other hand I haven&rsquo;t seen a single source that recommends running Redis Windows in production even though Microsoft claims that it&rsquo;s heavily tested and production ready. General sentiment is like &ldquo;why would you do that when you can just run it on a Linux VM&rdquo; which I can understand for SAAS scenarios.</p>

<p>For the project that I&rsquo;m working on there is also installation scenarios on customer premises so asking them to get a Linux machine is a bit over the top my friend says. So we&rsquo;re willing to give the Microsoft Redis a chance. Hey these guys are also running <a href="http://azure.microsoft.com/en-us/services/cache/" target="_blank">Redis on Azure</a> after all&hellip;</p>

<p><a href="https://servicestack.net/" target="_blank">ServiceStack</a> is one of the frameworks that put focus on Redis. Supporting Redis is one of their selling points:</p>

<p><img src="/assets/Playing_Around_With_Redis/ServiceStack.png" alt="ServiceStack products" /></p>

<p>ServiceStack also distributes <a href="https://github.com/ServiceStack/redis-windows" target="_blank">MS Open Tech Redis port of Windows on GitHub</a>. If you want to run the Windows version of Redis there are very useful information here on how to install Redis Windows as a service or how to use <a href="http://docs-v1.vagrantup.com/v1/docs/getting-started/index.html" target="_blank">Vagrant</a> to install the Linux version of Redis on a Windows box with virtualization through <a href="https://www.virtualbox.org/" target="_blank">VirtualBox</a>.</p>

<h2>Redis C# Clients</h2>

<p>There are <a href="http://redis.io/clients" target="_blank">a bunch of clients</a> out there implemented for Redis with various languages. As of this writing this is how the list looks like for C#. The ones that are starred are the recommended clients by Redis.</p>

<p><img src="/assets/Playing_Around_With_Redis/RedisCSharpClients.png" alt="Redis C# Clients" /></p>

<p>I&rsquo;ve picked the ServiceStack client over StackExchange because of the richer and nicer to use library. It looks like the StackExchange client is claiming to be the fastest though.</p>

<p>Right after starting to use the latest ServiceStack Redis client I&rsquo;ve noticed that they&rsquo;ve switched to a commercial license with version 4. But <a href="https://github.com/ServiceStackV3/ServiceStackV3" target="_blank">the previous version 3</a> continues to be BSD license. So I fell back to this version after finding that out. Version 3 is also a pretty complete library and for my current purposes it doesn&rsquo;t really matter.</p>

<p>Here is how the API looks like (click <a href="http://mono.servicestack.net/img/Redis-annotated.png" target="_blank">here</a> for a version that you can zoom):</p>

<p><img src="/assets/Playing_Around_With_Redis/Redis-annotated.png" alt="Redis Client API Overview" /></p>

<h2>A User Scenario</h2>

<p>One of the scenarios that can be implemented with this prototype is as follows (remember the architecture diagram from the beginning of this post):</p>

<ul>
<li>When a user adds a record to the system through the Web UI (management site) the App Server needs to know about that record without going to the database.</li>
<li>When something interesting happens on the App Server the Web UI needs to be notified and then it should refresh the information on the connected clients' browsers. A typical dashboard scenario which should not require manual page refresh.</li>
</ul>


<p>Thanks for reading.</p>

<p>Hakan</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Disaster Recovery on Azure]]></title>
    <link href="http://www.hakantuncer.com/blog/2014/04/27/disaster-recovery-on-azure/"/>
    <updated>2014-04-27T19:30:37+02:00</updated>
    <id>http://www.hakantuncer.com/blog/2014/04/27/disaster-recovery-on-azure</id>
    <content type="html"><![CDATA[<br/><br/>


<blockquote><h4>UPDATE:</h4>

<p>Things keep changing &amp; improving really FAST on Azure. This post may not reflect the latest capabilities in terms of Disaster Recovery for Azure Applications. Make sure to read the subject on <a href="https://msdn.microsoft.com/en-us/library/azure/dn251004.aspx">MSDN</a> as well.</p></blockquote>

<p>There are things we tend to ignore. Things that we don&rsquo;t want to spend time on or think about because there are no immediate benefits. Disaster recovery is one of those topics. The chances are fairly low, but the impact on our business is catastrophic. The consequences can go as far as losing the business completely. Unless there is some sort of a disaster recovery plan in place.</p>

<p>Because I&rsquo;m working with Microsoft Azure on a daily basis as part of my job, I&rsquo;ll be focusing on Azure but the fundamental principles and concepts should be universal for every cloud platform, or even for on premises type scenarios.</p>

<h2>Two key objectives</h2>

<br>


<blockquote><h5>Recovery Time Objective (RTO)</h5>

<p>Recovery Time Objective is the maximum amount of time allocated for restoring application functionality.</p></blockquote>

<p>This is usually a requirement coming from the business. Basically the question is how critical is your platform and how much down time can you tolerate in such a catastrophic event?</p>

<blockquote><h5>Recovery Point Objective (RPO)</h5>

<p>The recovery point objective (RPO) is the acceptable time window of lost data due to the recovery process.</p></blockquote>

<p>For example, if the RPO is one hour, you must completely back up or replicate the data at least every hour. Once you bring up the application in an alternate datacenter, the backup data may be missing up to an hour of data. Like RTO, critical applications target a much smaller RPO.</p>

<p>These two key objectives determine the approach that needs to be followed and therefore the effort and cost of the whole disaster recovery plan.</p>

<p>The purpose of this blog post is not to describe solutions for each possible combination of these two objectives, but to describe a couple of approaches that can tolerate relatively long recovery time objective (~24 hours) with a more aggressive (so short) recovery point objective.</p>

<p>Don&rsquo;t forget that having a short RTO and RPO together can be both costly and complex to implement depending on the storage needs of your applications.</p>

<h2>Running business on a single datacenter</h2>

<p>Let&rsquo;s say you haven&rsquo;t thought on a disaster recovery plan yet. If your software is deployed on a single datacenter and making use of Azure Storage and/or Azure SQL Database you are at risk of losing your business should your datacenter goes dark due to a disaster or a catastrophic failure.</p>

<p>An Azure datacenter is equipped with fault domains and redundancy to keep your service highly available but these are all inside the datacenter. If the whole datacenter goes down all the compute instances, databases and storage services will go down with it.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Single_Region_Deployment.png" width="500" height="438" title="&lsquo;Single Region Deployment&rsquo;" ></p>

<p>Assuming that you still have your software in-house somewhere, there should be no risk of losing the compute instances forever. By creating and deploying the cloud packages on another datacenter the compute instances can be recovered. <strong>The key to business continuity is to be able to recover the data that is stored on Azure Storage and Azure SQL Databases.</strong></p>

<h2>Frequently backup data outside the datacenter</h2>

<p>Whatever the recovery objectives are, backing up application data outside of the datacenter is a must for business continuity. How frequently the data is backed up or synced outside the datacenter will determine the recovery point objective.</p>

<p>When there are backups available outside the datacenter, the environment can be moved to another datacenter by restoring the data and installing compute instances using existing cloud service packages.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Redeploy_Azure_Datacenter.png" alt="Redeploy to another datacenter" /></p>

<p>This is called &ldquo;redeploy&rdquo; recovery model and as you might already guess has a long recovery time objective. All the individual pieces of the environment needs to be moved to another datacenter and redeployed.</p>

<h2>How to backup Azure data (together with RPO and RTO considerations)</h2>

<br><br>


<h4>1. Azure Storage</h4>

<p>The good news is that Azure Storage Service has built-in replication strategies. Two of them are geographical redundancy, meaning that all storage data is replicated across datacenters. That&rsquo;s exactly what disaster recovery is about.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Azure_Storage_Redundancy.png" alt="Azure Storage Geo Redundancy" /></p>

<p>The difference between &ldquo;Geo Redundant&rdquo; and &ldquo;Read-Access Geo Redundant&rdquo; is that the latter allows the redundant data to be accessed at all times in a read-only fashion. This brings us to another important point:</p>

<blockquote><p>Here is what Microsoft&rsquo;s documentation say about the estimated azure storage failover time in case of a disaster:  &ldquo;estimated time that the data will be accessible to customers after a disaster is 24 hours.&rdquo;</p></blockquote>

<p>In the light of these information 3 options appear for backing up the Azure Storage:</p>

<ul>
<li><strong>Geo-Redundancy</strong>: There is no SLA but RPO is practically very short. RTO is long. Only after about 24 hours the failover process will be completed by Microsoft and the data will be available again.</li>
<li><strong>Read-Access Geo Redundant</strong>: RPO is again the same as above. However almost immediately the redundant data can be accessed (read-only) which allows systems to run in a degraded mode (if they&rsquo;re designed in such a fault tolerant way of course). Then again in 24 hours everything will be fully operational.</li>
<li><strong>Custom</strong>: If RTO needs to be shorter, the only option is to look for a third party product that can replicate an azure storage to another datacenter. This redundant storage will always be available in Read/Write mode (since it&rsquo;s just another regular storage account, only in another geographical region).</li>
</ul>


<br>


<h4>2. Azure SQL Databases</h4>

<p>Yesterday I woke up to an <a href="http://view.email.microsoftemail.com/?j=fe9016787161057a71&amp;m=fe621570756503797d1c&amp;ls=fe1817787c60027e711d79&amp;l=fec21c767365017e&amp;s=fe2212717465037a701d78&amp;jb=ff5e177873&amp;ju=fe5711777060017b7611">annoucement from Microsoft</a> introducing new types of databases in Azure with new disaster recovery features:</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/DisasterRecoveryPerDatabaseType.png" alt="Disaster Recovery Per Database Type" /></p>

<p>Let&rsquo;s look at each option and see what it means:</p>

<ul>
<li><strong>Restore to an alternate Azure region</strong>: This phrase means &ldquo;no silver bullet&rdquo;. Basic type database owners are responsible with their own backup and restore operations. Luckily there is a convenient way to backup and restore an Azure SQL Database regardless of its type.</li>
</ul>


<br>


<h5>Automatic Database Export:</h5>

<p>Actually there are a couple of ways to replicate or backup a database. But most of these options have their own shortcomings for disaster recovery. There is a feature called <a href="http://msdn.microsoft.com/en-US/library/azure/ff951624.aspx">Database Copy</a> which creates a <a href="http://technet.microsoft.com/en-us/library/ms151176.aspx">transactionally consistent replica</a> of the source database in <strong>the same datacenter.</strong> Because the replica resides in the same datacenter there is no geo-redundancy. But after the copying is completed, this database can be exported to a storage account in another datacenter.</p>

<p>This is exactly what <strong>Automatic Database Export</strong> feature does. It first replicates the database with a copy operation, thus getting a transactionally consistent copy of the database, then exports it to the storage account that is configured. To see how it is configured you can visit <a href="http://blogs.msdn.com/b/sql-bi-sap-cloud-crm_all_in_one_place/archive/2013/07/24/sql-azure-automated-database-export.aspx">this blog post</a>.</p>

<p>A direct manual export operation itself does not generate a transactionally consistent copy of a database. This means you may end up having an Order item in the database with a missing OrderDetails. Automatic Database Export however takes care of this problem. The documentation was not very clear on that so I asked the man himself:</p>

<br>




<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/hakant">@hakant</a> <a href="https://twitter.com/Azure">@Azure</a> yes I&#39;m pretty sure it is</p>&mdash; Scott Guthrie (@scottgu) <a href="https://twitter.com/scottgu/statuses/460180886915276801">April 26, 2014</a></blockquote>


<script async src="http://www.hakantuncer.com//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>So we&rsquo;re on the right track here.</p>

<br>


<blockquote><p><strong>Attention:</strong> First part of Automatic Database Export is the replication of the database with Database COPY operation which runs two databases at the same time. This means export operations are partially doubling the database costs.</p></blockquote>

<p>So far so good&hellip; But what determines RTO and RPO for Automatic Database Export? This analysis depends on the frequency and the destination storage configured for the database export operation.</p>

<br>


<ul>
<li><em>Exporting to a separate storage account on another datacenter:</em></li>
</ul>


<p>In this scenario, the RPO will simply be around [Database Export Frequency]. If the database is exported every 6 hours, in the worst case scenario 6 hours of data may be lost.</p>

<p>RTO is simply the [Database Restore Duration]. No need to wait for Microsoft to execute the Azure Storage failover process. The database export file is immediately available on the storage account in the other datacenter.</p>

<br>


<ul>
<li><em>Exporting to the primary storage account on the same datacenter:</em></li>
</ul>


<p>In this scenario, we&rsquo;re relying on the Automated Azure Storage Geo-Replication to get the exported database transferred to another datacenter. Since this scenario uses the geo-replication feature of the storage account there won&rsquo;t be any additional costs for geo-reduntant storage. This is nice indeed.</p>

<p>But in the worst case scenario the RPO can go as far as [2 * Database Export Frequency] if the datacenter happens to fail during the geo-replication process. In that case, the last export file won&rsquo;t be available after the failure but only the one before that will be.</p>

<p>On the other hand, the RTO is [Estimated Azure Storage Geo-Failover Time + Database Restore Duration]. Again, Microsoft&rsquo;s estimation for Azure Storage geo-failover is around 24 hours.</p>

<p>So the moral of the story is that having a dedicated separate Azure Storage account on the other datacenter is beneficial for both RPO and RTO. But as usual the downside is financial. The separate storage account is also billed separately. Both for storage and bandwith to transfer the data. Though it&rsquo;s also worth mentioning that Azure Storage is <a href="http://azure.microsoft.com/en-us/pricing/details/storage/">fairly cheap</a>.</p>

<p>Now let&rsquo;s move to the other disaster recovery features.</p>

<ul>
<li><p><strong>Geo-Replication Passive Replica</strong>: Unfortunately there is no documentation available for this mysterious feature yet. Frankly there is no single mention of this feature anywhere else at the time of this writing. So the technique described above for the Basic databases also applies here until Microsoft actually ships this feature or shares more about it. I&rsquo;ll update this section when that happens.</p></li>
<li><p><strong>Active Geo-Replication</strong>: This one is a silver bullet solution. Unfortunately only the Premium database owners can make use of this feature. With Active Geo-Replication, you can create and maintain up to four readable secondary databases across geographic regions. Basically this gives a very short RPO and RTO for the price of running multiple Premium databases (<a href="http://azure.microsoft.com/en-us/pricing/details/sql-database/#basic-standard-and-premium">which is quite a lot</a>). You can read more about <a href="http://msdn.microsoft.com/en-US/library/azure/dn741339.aspx">Active Geo-Replication on MSDN</a>.</p></li>
</ul>


<h2>Final thoughts</h2>

<p>If your business can tolerate larger than ~24 hrs recovery time objective (RTO) Azure provides a couple of inexpensive features that you can already start making use of. These features also require very little effort to setup and you guarantee business continuity in case of a catastrophic event.</p>

<p>The most cost effective strategy is the following:</p>

<ol>
<li>Make sure your storage account is configured for Geo-Redundancy (default).</li>
<li>Turn on Automatic Database Export and configure a high frequency that makes sense. Keep track of how long this operation takes and adjust your frequency based on that as well.</li>
<li>Set your default azure storage account as the destination of automatic database export. So rely on the geo-redundancy of your storage account.</li>
</ol>


<p>If you need a shorter RTO build up on this strategy. This post is mainly focused on &ldquo;Redeploy&rdquo; pattern. There are other patterns available on MSDN and some are focused on more aggressive RTO&rsquo;s. I recommend reading: <a href="http://msdn.microsoft.com/en-us/library/dn251004.aspx">Disaster Recovery and High Availability for Azure Applications</a>.</p>

<h2>References</h2>

<p>Most information on this blog post is obtained from the following MSDN pages. Some others are direct observations &amp; usage from Azure Management Portal.</p>

<ul>
<li><a href="http://msdn.microsoft.com/en-us/library/dn251004.aspx">Disaster Recovery and High Availability for Azure Applications</a></li>
<li><a href="http://msdn.microsoft.com/library/azure/hh852669.aspx">Azure SQL Database Business Continuity</a></li>
<li><a href="http://msdn.microsoft.com/en-US/library/azure/ff951624.aspx">Copying Databases in Azure SQL Database</a></li>
<li><a href="http://msdn.microsoft.com/en-US/library/azure/hh335292.aspx">How to: Import and Export a Database (Azure SQL Database)</a></li>
<li><a href="http://blogs.msdn.com/b/windowsazure/archive/2014/04/04/sql-database-updates-coming-soon-to-the-premium-preview.aspx">SQL Database updates coming soon to the Premium preview</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
