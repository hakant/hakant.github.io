<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Architecture | Hakan Tuncer's Blog]]></title>
  <link href="http://www.hakantuncer.com/blog/categories/architecture/atom.xml" rel="self"/>
  <link href="http://www.hakantuncer.com/"/>
  <updated>2015-05-05T10:23:44+02:00</updated>
  <id>http://www.hakantuncer.com/</id>
  <author>
    <name><![CDATA[Hakan Tuncer]]></name>
    <email><![CDATA[hakantuncer@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Playing Around With Redis & SignalR - (Part 1)]]></title>
    <link href="http://www.hakantuncer.com/blog/2014/10/26/playing-around-with-redis-and-signalr-part-1/"/>
    <updated>2014-10-26T19:32:27+01:00</updated>
    <id>http://www.hakantuncer.com/blog/2014/10/26/playing-around-with-redis-and-signalr-part-1</id>
    <content type="html"><![CDATA[<p>A good friend of mine contacted me a couple of weeks ago and asked if I would accept a freelancing project. Around research and prototyping. Possibly leading to a new architecture for their existing product. The existing architecture didn&rsquo;t allow any further room for the new requirements they had &ndash; he told me during our skype conversation. I thought it was fun and took the offer. R&amp;D and freelancing.. not bad huh!</p>

<p>So as part of the assignment I&rsquo;ve been doing some interesting reasearch and prototyping so why not blog about it as well I thought. As long as no confidential information is shared it should be OK. This can easily become a couple of posts. So I&rsquo;ll call this Part 1.</p>

<h2>Going technical:</h2>

<p>One of the requirements is to decouple the system components so that they can be deployed and run on different boxes. So a typical &ldquo;scaling out&rdquo; problem.</p>

<p>The other requirement is to enable these components to talk to each other about the changes happening through-out the system. So these components have to be able to send/receive messages to/from each other. This is especially important after separating these components obviously.</p>

<p>After studying the existing architecture and reading through some code, this was my initial design. I&rsquo;ve changed the application names to something more generic to keep it confidential but the technical idea behind it is still the same:</p>

<p><img src="/assets/Playing_Around_With_Redis/SystemArchitectureDiagram.png" alt="System Architecture Diagram" /></p>

<p>Let&rsquo;s look at some of the key components:</p>

<ul>
<li><strong>App Server</strong>: The application server is running a mission critical service that needs to be very fast. It is actually orchestrating incoming and outgoing voice calls, can&rsquo;t afford to be slow. Obviously there is also some bookkeeping that the application server does. The states of the calls and users change over time and this kind of application state information is also interesting to the other parts of the system for various reasons like monitoring, dashboards or some other type of actions that needs to be taken.</li>
<li><strong>Web UI</strong>: This is basically where customers will sign in and manage their system and data. They can do administrative work as well as look at various dashboards to see what is going on. This is going to become a self-hosted (yes OWIN) MVC 5 application.</li>
<li><strong>Web API</strong>: Most things that can be done with the Web UI should be done with API as well. So that customers can be creative and build their own solutions &amp; extensions. This will be built with Web API 2.0 and self hosted.</li>
</ul>


<p>I will tell more about the other components in the future posts. But careful readers should have already noticed that some of the requirements beg for a &ldquo;message broker&rdquo; / &ldquo;service bus&rdquo; type of a system in between to coordinate messaging.
Let&rsquo;s talk a bit about <a href="http://redis.io/" target="_blank">Redis</a> now.</p>

<h2>Redis:</h2>

<p>Here is the official definiton of Redis:</p>

<blockquote><p>Redis is an open source, BSD licensed, advanced key-value cache and store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets, sorted sets, bitmaps and hyperloglogs.</p></blockquote>

<p>At <a href="http://www.niposoftware.com/" target="_blank">NIPO Software</a> we&rsquo;ve been using Redis for a while but frankly I didn&rsquo;t know that Redis was also supporting pub/sub messaging. As a solution the first thing that came to my mind was <a href="http://www.rabbitmq.com/" target="_blank">RabbitMQ</a>, which is a message broker. But Redis can do other interesting things as well. So I focused on Redis instead.</p>

<p>Here is what Wikipedia says about the &ldquo;Publish–subscribe pattern&rdquo;:</p>

<blockquote><p>In software architecture, publish–subscribe is a messaging pattern where senders of messages, called publishers, do not program the messages to be sent directly to specific receivers, called subscribers. Instead, published messages are characterized into classes, without knowledge of what, if any, subscribers there may be. Similarly, subscribers express interest in one or more classes, and only receive messages that are of interest, without knowledge of what, if any, publishers there are.</p></blockquote>

<p>And what Redis adds to it:</p>

<blockquote><p>This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology.</p></blockquote>

<p>Redis is also a cache store and a persistent storage. If you tell Redis to do so, it will take occasional snapshots of the data in it&rsquo;s memory and persist it to disk. As you progress through the Redis documentation it feels like Redis is all you need : ) F.ex here is where Redis snapshotting feature is configured:</p>

<p>```bash</p>

<h6>########################## SNAPSHOTTING</h6>

<p>#</p>

<h1>Save the DB on disk:</h1>

<p>#</p>

<h1>save <seconds> <changes></h1>

<p>#</p>

<h1>Will save the DB if both the given number of seconds and the given</h1>

<h1>number of write operations against the DB occurred.</h1>

<p>#</p>

<h1>In the example below the behaviour will be to save:</h1>

<h1>after 900 sec (15 min) if at least 1 key changed</h1>

<h1>after 300 sec (5 min) if at least 10 keys changed</h1>

<h1>after 60 sec if at least 10000 keys changed</h1>

<p>#</p>

<h1>Note: you can disable saving at all commenting all the &ldquo;save&rdquo; lines.</h1>

<p>#</p>

<h1>It is also possible to remove all the previously configured save</h1>

<h1>points by adding a save directive with a single empty string argument</h1>

<h1>like in the following example:</h1>

<p>#</p>

<h1>save &ldquo;&rdquo;</h1>

<h1>save 900 1</h1>

<h1>save 300 10</h1>

<h1>save 60 10000</h1>

<p>```</p>

<p>Depending on how critical your data is you can specify different levels of aggressiveness. As with everything the more aggressively you save the more stress you put on the system. On the other hand you can disable saving completely which will make Redis a purely caching server.</p>

<p>If you want to quickly get up and running with Redis I recommend the <a href="http://www.pluralsight.com/courses/building-nosql-apps-redis" target="_blank">Pluralsight course from John Sonmez</a>. It does a great job on getting you from zero to sixty.</p>

<h2>Redis on Windows</h2>

<p>Redis is an open source project and only supports Linux. But Microsoft Open Tech group develops and supports a <a href="https://github.com/MSOpenTech/redis" target="_blank">Windows port targeting Win64</a>. On the other hand I haven&rsquo;t seen a single source that recommends running Redis Windows in production even though Microsoft claims that it&rsquo;s heavily tested and production ready. General sentiment is like &ldquo;why would you do that when you can just run it on a Linux VM&rdquo; which I can understand for SAAS scenarios.</p>

<p>For the project that I&rsquo;m working on there is also installation scenarios on customer premises so asking them to get a Linux machine is a bit over the top my friend says. So we&rsquo;re willing to give the Microsoft Redis a chance. Hey these guys are also running <a href="http://azure.microsoft.com/en-us/services/cache/" target="_blank">Redis on Azure</a> after all&hellip;</p>

<p><a href="https://servicestack.net/" target="_blank">ServiceStack</a> is one of the frameworks that put focus on Redis. Supporting Redis is one of their selling points:</p>

<p><img src="/assets/Playing_Around_With_Redis/ServiceStack.png" alt="ServiceStack products" /></p>

<p>ServiceStack also distributes <a href="https://github.com/ServiceStack/redis-windows" target="_blank">MS Open Tech Redis port of Windows on GitHub</a>. If you want to run the Windows version of Redis there are very useful information here on how to install Redis Windows as a service or how to use <a href="http://docs-v1.vagrantup.com/v1/docs/getting-started/index.html" target="_blank">Vagrant</a> to install the Linux version of Redis on a Windows box with virtualization through <a href="https://www.virtualbox.org/" target="_blank">VirtualBox</a>.</p>

<h2>Redis C# Clients</h2>

<p>There are <a href="http://redis.io/clients" target="_blank">a bunch of clients</a> out there implemented for Redis with various languages. As of this writing this is how the list looks like for C#. The ones that are starred are the recommended clients by Redis.</p>

<p><img src="/assets/Playing_Around_With_Redis/RedisCSharpClients.png" alt="Redis C# Clients" /></p>

<p>I&rsquo;ve picked the ServiceStack client over StackExchange because of the richer and nicer to use library. It looks like the StackExchange client is claiming to be the fastest though.</p>

<p>Right after starting to use the latest ServiceStack Redis client I&rsquo;ve noticed that they&rsquo;ve switched to a commercial license with version 4. But <a href="https://github.com/ServiceStackV3/ServiceStackV3" target="_blank">the previous version 3</a> continues to be BSD license. So I fell back to this version after finding that out. Version 3 is also a pretty complete library and for my current purposes it doesn&rsquo;t really matter.</p>

<p>Here is how the API looks like (click <a href="http://mono.servicestack.net/img/Redis-annotated.png" target="_blank">here</a> for a version that you can zoom):</p>

<p><img src="/assets/Playing_Around_With_Redis/Redis-annotated.png" alt="Redis Client API Overview" /></p>

<h2>A User Scenario</h2>

<p>One of the scenarios that needs to be implemented with this prototype is as follows (remember the architecture diagram from the beginning of this post):</p>

<ul>
<li>When a user adds a record to the system through the Web UI (management site) the App Server needs to know about that record without going to the database.</li>
<li>When something interesting happens on the App Server the Web UI needs to be notified and then it should refresh the information on the connected clients' browsers. A typical dashboard scenario which should not require manual page refresh.</li>
</ul>


<p>In the next blog post I&rsquo;ll write about how to implement these features using Redis and SignalR.</p>

<p>Until then, take care.</p>

<p>Hakan</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Disaster Recovery on Azure]]></title>
    <link href="http://www.hakantuncer.com/blog/2014/04/27/disaster-recovery-on-azure/"/>
    <updated>2014-04-27T19:30:37+02:00</updated>
    <id>http://www.hakantuncer.com/blog/2014/04/27/disaster-recovery-on-azure</id>
    <content type="html"><![CDATA[<p>There are things we tend to ignore. Things that we don&rsquo;t want to spend time on or think about because there are no immediate benefits. Disaster recovery is one of those topics. The chances are fairly low, but the impact on our business is catastrophic. The consequences can go as far as losing the business completely. Unless there is some sort of a disaster recovery plan in place.</p>

<p>Because I&rsquo;m working with Microsoft Azure on a daily basis as part of my job, I&rsquo;ll be focusing on Azure but the fundamental principles and concepts should be universal for every cloud platform, or even for on premises type scenarios.</p>

<h2>Two key objectives</h2>

<br>


<blockquote><h5>Recovery Time Objective (RTO)</h5>

<p>Recovery Time Objective is the maximum amount of time allocated for restoring application functionality.</p></blockquote>

<p>This is usually a requirement coming from the business. Basically the question is how critical is your platform and how much down time can you tolerate in such a catastrophic event?</p>

<blockquote><h5>Recovery Point Objective (RPO)</h5>

<p>The recovery point objective (RPO) is the acceptable time window of lost data due to the recovery process.</p></blockquote>

<p>For example, if the RPO is one hour, you must completely back up or replicate the data at least every hour. Once you bring up the application in an alternate datacenter, the backup data may be missing up to an hour of data. Like RTO, critical applications target a much smaller RPO.</p>

<p>These two key objectives determine the approach that needs to be followed and therefore the effort and cost of the whole disaster recovery plan.</p>

<p>The purpose of this blog post is not to describe solutions for each possible combination of these two objectives, but to describe a couple of approaches that can tolerate relatively long recovery time objective (~24 hours) with a more aggressive (so short) recovery point objective.</p>

<p>Don&rsquo;t forget that having a short RTO and RPO together can be both costly and complex to implement depending on the storage needs of your applications.</p>

<h2>Running business on a single datacenter</h2>

<p>Let&rsquo;s say you haven&rsquo;t thought on a disaster recovery plan yet. If your software is deployed on a single datacenter and making use of Azure Storage and/or Azure SQL Database you are at risk of losing your business should your datacenter goes dark due to a disaster or a catastrophic failure.</p>

<p>An Azure datacenter is equipped with fault domains and redundancy to keep your service highly available but these are all inside the datacenter. If the whole datacenter goes down all the compute instances, databases and storage services will go down with it.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Single_Region_Deployment.png" width="500" height="438" title="&lsquo;Single Region Deployment&rsquo;" ></p>

<p>Assuming that you still have your software in-house somewhere, there should be no risk of losing the compute instances forever. By creating and deploying the cloud packages on another datacenter the compute instances can be recovered. <strong>The key to business continuity is to be able to recover the data that is stored on Azure Storage and Azure SQL Databases.</strong></p>

<h2>Frequently backup data outside the datacenter</h2>

<p>Whatever the recovery objectives are, backing up application data outside of the datacenter is a must for business continuity. How frequently the data is backed up or synced outside the datacenter will determine the recovery point objective.</p>

<p>When there are backups available outside the datacenter, the environment can be moved to another datacenter by restoring the data and installing compute instances using existing cloud service packages.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Redeploy_Azure_Datacenter.png" alt="Redeploy to another datacenter" /></p>

<p>This is called &ldquo;redeploy&rdquo; recovery model and as you might already guess has a long recovery time objective. All the individual pieces of the environment needs to be moved to another datacenter and redeployed.</p>

<h2>How to backup Azure data (together with RPO and RTO considerations)</h2>

<br><br>


<h4>1. Azure Storage</h4>

<p>The good news is that Azure Storage Service has built-in replication strategies. Two of them are geographical redundancy, meaning that all storage data is replicated across datacenters. That&rsquo;s exactly what disaster recovery is about.</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/Azure_Storage_Redundancy.png" alt="Azure Storage Geo Redundancy" /></p>

<p>The difference between &ldquo;Geo Redundant&rdquo; and &ldquo;Read-Access Geo Redundant&rdquo; is that the latter allows the redundant data to be accessed at all times in a read-only fashion. This brings us to another important point:</p>

<blockquote><p>Here is what Microsoft&rsquo;s documentation say about the estimated azure storage failover time in case of a disaster:  &ldquo;estimated time that the data will be accessible to customers after a disaster is 24 hours.&rdquo;</p></blockquote>

<p>In the light of these information 3 options appear for backing up the Azure Storage:</p>

<ul>
<li><strong>Geo-Redundancy</strong>: There is no SLA but RPO is practically very short. RTO is long. Only after about 24 hours the failover process will be completed by Microsoft and the data will be available again.</li>
<li><strong>Read-Access Geo Redundant</strong>: RPO is again the same as above. However almost immediately the redundant data can be accessed (read-only) which allows systems to run in a degraded mode (if they&rsquo;re designed in such a fault tolerant way of course). Then again in 24 hours everything will be fully operational.</li>
<li><strong>Custom</strong>: If RTO needs to be shorter, the only option is to look for a third party product that can replicate an azure storage to another datacenter. This redundant storage will always be available in Read/Write mode (since it&rsquo;s just another regular storage account, only in another geographical region).</li>
</ul>


<br>


<h4>2. Azure SQL Databases</h4>

<p>Yesterday I woke up to an <a href="http://view.email.microsoftemail.com/?j=fe9016787161057a71&amp;m=fe621570756503797d1c&amp;ls=fe1817787c60027e711d79&amp;l=fec21c767365017e&amp;s=fe2212717465037a701d78&amp;jb=ff5e177873&amp;ju=fe5711777060017b7611">annoucement from Microsoft</a> introducing new types of databases in Azure with new disaster recovery features:</p>

<p><img src="/assets/Disaster_Recovery_On_Azure/DisasterRecoveryPerDatabaseType.png" alt="Disaster Recovery Per Database Type" /></p>

<p>Let&rsquo;s look at each option and see what it means:</p>

<ul>
<li><strong>Restore to an alternate Azure region</strong>: This phrase means &ldquo;no silver bullet&rdquo;. Basic type database owners are responsible with their own backup and restore operations. Luckily there is a convenient way to backup and restore an Azure SQL Database regardless of its type.</li>
</ul>


<br>


<h5>Automatic Database Export:</h5>

<p>Actually there are a couple of ways to replicate or backup a database. But most of these options have their own shortcomings for disaster recovery. There is a feature called <a href="http://msdn.microsoft.com/en-US/library/azure/ff951624.aspx">Database Copy</a> which creates a <a href="http://technet.microsoft.com/en-us/library/ms151176.aspx">transactionally consistent replica</a> of the source database in <strong>the same datacenter.</strong> Because the replica resides in the same datacenter there is no geo-redundancy. But after the copying is completed, this database can be exported to a storage account in another datacenter.</p>

<p>This is exactly what <strong>Automatic Database Export</strong> feature does. It first replicates the database with a copy operation, thus getting a transactionally consistent copy of the database, then exports it to the storage account that is configured. To see how it is configured you can visit <a href="http://blogs.msdn.com/b/sql-bi-sap-cloud-crm_all_in_one_place/archive/2013/07/24/sql-azure-automated-database-export.aspx">this blog post</a>.</p>

<p>A direct manual export operation itself does not generate a transactionally consistent copy of a database. This means you may end up having an Order item in the database with a missing OrderDetails. Automatic Database Export however takes care of this problem. The documentation was not very clear on that so I asked the man himself:</p>

<br>




<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/hakant">@hakant</a> <a href="https://twitter.com/Azure">@Azure</a> yes I&#39;m pretty sure it is</p>&mdash; Scott Guthrie (@scottgu) <a href="https://twitter.com/scottgu/statuses/460180886915276801">April 26, 2014</a></blockquote>


<script async src="http://www.hakantuncer.com//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>So we&rsquo;re on the right track here.</p>

<br>


<blockquote><p><strong>Attention:</strong> First part of Automatic Database Export is the replication of the database with Database COPY operation which runs two databases at the same time. This means export operations are partially doubling the database costs.</p></blockquote>

<p>So far so good&hellip; But what determines RTO and RPO for Automatic Database Export? This analysis depends on the frequency and the destination storage configured for the database export operation.</p>

<br>


<ul>
<li><em>Exporting to a separate storage account on another datacenter:</em></li>
</ul>


<p>In this scenario, the RPO will simply be around [Database Export Frequency]. If the database is exported every 6 hours, in the worst case scenario 6 hours of data may be lost.</p>

<p>RTO is simply the [Database Restore Duration]. No need to wait for Microsoft to execute the Azure Storage failover process. The database export file is immediately available on the storage account in the other datacenter.</p>

<br>


<ul>
<li><em>Exporting to the primary storage account on the same datacenter:</em></li>
</ul>


<p>In this scenario, we&rsquo;re relying on the Automated Azure Storage Geo-Replication to get the exported database transferred to another datacenter. Since this scenario uses the geo-replication feature of the storage account there won&rsquo;t be any additional costs for geo-reduntant storage. This is nice indeed.</p>

<p>But in the worst case scenario the RPO can go as far as [2 * Database Export Frequency] if the datacenter happens to fail during the geo-replication process. In that case, the last export file won&rsquo;t be available after the failure but only the one before that will be.</p>

<p>On the other hand, the RTO is [Estimated Azure Storage Geo-Failover Time + Database Restore Duration]. Again, Microsoft&rsquo;s estimation for Azure Storage geo-failover is around 24 hours.</p>

<p>So the moral of the story is that having a dedicated separate Azure Storage account on the other datacenter is beneficial for both RPO and RTO. But as usual the downside is financial. The separate storage account is also billed separately. Both for storage and bandwith to transfer the data. Though it&rsquo;s also worth mentioning that Azure Storage is <a href="http://azure.microsoft.com/en-us/pricing/details/storage/">fairly cheap</a>.</p>

<p>Now let&rsquo;s move to the other disaster recovery features.</p>

<ul>
<li><p><strong>Geo-Replication Passive Replica</strong>: Unfortunately there is no documentation available for this mysterious feature yet. Frankly there is no single mention of this feature anywhere else at the time of this writing. So the technique described above for the Basic databases also applies here until Microsoft actually ships this feature or shares more about it. I&rsquo;ll update this section when that happens.</p></li>
<li><p><strong>Active Geo-Replication</strong>: This one is a silver bullet solution. Unfortunately only the Premium database owners can make use of this feature. With Active Geo-Replication, you can create and maintain up to four readable secondary databases across geographic regions. Basically this gives a very short RPO and RTO for the price of running multiple Premium databases (<a href="http://azure.microsoft.com/en-us/pricing/details/sql-database/#basic-standard-and-premium">which is quite a lot</a>). You can read more about <a href="http://msdn.microsoft.com/en-US/library/azure/dn741339.aspx">Active Geo-Replication on MSDN</a>.</p></li>
</ul>


<h2>Final thoughts</h2>

<p>If your business can tolerate larger than ~24 hrs recovery time objective (RTO) Azure provides a couple of inexpensive features that you can already start making use of. These features also require very little effort to setup and you guarantee business continuity in case of a catastrophic event.</p>

<p>The most cost effective strategy is the following:</p>

<ol>
<li>Make sure your storage account is configured for Geo-Redundancy (default).</li>
<li>Turn on Automatic Database Export and configure a high frequency that makes sense. Keep track of how long this operation takes and adjust your frequency based on that as well.</li>
<li>Set your default azure storage account as the destination of automatic database export. So rely on the geo-redundancy of your storage account.</li>
</ol>


<p>If you need a shorter RTO build up on this strategy. This post is mainly focused on &ldquo;Redeploy&rdquo; pattern. There are other patterns available on MSDN and some are focused on more aggressive RTO&rsquo;s. I recommend reading: <a href="http://msdn.microsoft.com/en-us/library/dn251004.aspx">Disaster Recovery and High Availability for Azure Applications</a>.</p>

<h2>References</h2>

<p>Most information on this blog post is obtained from the following MSDN pages. Some others are direct observations &amp; usage from Azure Management Portal.</p>

<ul>
<li><a href="http://msdn.microsoft.com/en-us/library/dn251004.aspx">Disaster Recovery and High Availability for Azure Applications</a></li>
<li><a href="http://msdn.microsoft.com/library/azure/hh852669.aspx">Azure SQL Database Business Continuity</a></li>
<li><a href="http://msdn.microsoft.com/en-US/library/azure/ff951624.aspx">Copying Databases in Azure SQL Database</a></li>
<li><a href="http://msdn.microsoft.com/en-US/library/azure/hh335292.aspx">How to: Import and Export a Database (Azure SQL Database)</a></li>
<li><a href="http://blogs.msdn.com/b/windowsazure/archive/2014/04/04/sql-database-updates-coming-soon-to-the-premium-preview.aspx">SQL Database updates coming soon to the Premium preview</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
